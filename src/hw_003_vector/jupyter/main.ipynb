{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ff08f",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b525a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d519561",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "284baf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'path.dataset.original': './IMDB Dataset.csv',\n",
    "            'path.dataset.prepared': './prepared IMDB Dataset.csv',\n",
    "            'path.dataset.w2v-embedding': './word2vec-google-news-300.model',\n",
    "\n",
    "            'train-test-split.test-size': 0.2,\n",
    "            'train-test-split.random-stage': 42,\n",
    "\n",
    "            'vec.tfidf.max-features': 300,\n",
    "            'vec.tfidf.norm': None,\n",
    "            'vec.tfidf.max-df': 0.95,\n",
    "            'vec.tfidf.min-df': 5,\n",
    "            'vec.tfidf.stop-words': 'english',\n",
    "\n",
    "            'hyper-params.on': False,\n",
    "            'hyper-params.cv': 3,\n",
    "            'hyper-params.scoring': 'accuracy',\n",
    "            'hyper-params.verbose': 3,\n",
    "            'hyper-params.jobs': -1,\n",
    "            'hyper-params.param-grid': {\n",
    "                'max_depth': [1, 3, None],\n",
    "                'n_estimators': [100, 200, 300]    \n",
    "            }\n",
    "#             'hyper-params.param-grid': {\n",
    "#                 'max_depth': [1],\n",
    "#                 'n_estimators': [100],\n",
    "#             }\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args) == 0 or args[0] not in self._params:\n",
    "            return None\n",
    "        return self._params[args[0]]\n",
    "\n",
    "conf = Configurator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09733e",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8e953b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_num(sentiment: str) -> int:\n",
    "    s = sentiment.lower()\n",
    "    if s == 'positive':\n",
    "        return 1\n",
    "    elif s == 'negative':\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "def num_to_sentiment(num: int) -> int:\n",
    "    return 'positive' if num == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6beb7",
   "metadata": {},
   "source": [
    "## Files checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e22f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(conf('path.dataset.original')):\n",
    "    print('Dataset is absent: ' + conf('path.dataset.original'))\n",
    "    \n",
    "embedding_path = conf('path.dataset.w2v-embedding')\n",
    "if not os.path.isfile(embedding_path):\n",
    "    print('Word2vec embedding is absent: ' + embedding_path + '; embedding source: https://huggingface.co/fse/word2vec-google-news-300/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e42b7",
   "metadata": {},
   "source": [
    "## Reset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1b2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "raw_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8eafe3",
   "metadata": {},
   "source": [
    "## Load preapared dataset if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13b8dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(conf('path.dataset.prepared'), encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        dataset = [{'review': item['review'], 'sentiment': int(item['sentiment'])} for item in reader]\n",
    "        print('Prepared dataset loaded')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015b2bd",
   "metadata": {},
   "source": [
    "## Load and prepare dataset if prepared does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76e1d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset exists\n"
     ]
    }
   ],
   "source": [
    "if dataset is None:\n",
    "    \n",
    "    tag_filter = re.compile(r'</?[a-z][\\w=\" -]*/?>')\n",
    "    punctuation_filter = re.compile(r'[.,!?*_)(]+')\n",
    "    space_filter = re.compile(r'\\s+')\n",
    "    \n",
    "    def filter_text(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = tag_filter.sub(' ', text)\n",
    "        text = punctuation_filter.sub(' ', text)\n",
    "        text = space_filter.sub(' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        with open(conf('path.dataset.original'), encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            raw_dataset = [item for item in reader]\n",
    "        dataset = []\n",
    "        print('Dataset loaded')\n",
    "\n",
    "        for datum in raw_dataset:\n",
    "            num_sentiment = sentiment_to_num(datum['sentiment'])\n",
    "            if num_sentiment != -1:\n",
    "                dataset.append({\n",
    "                    'review': filter_text(datum['review']),\n",
    "                    'sentiment': num_sentiment\n",
    "                })\n",
    "\n",
    "        with open(conf('path.dataset.prepared'), 'w', encoding='utf-8') as file:\n",
    "            fieldnames = ['review', 'sentiment']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            [writer.writerow(datum) for datum in dataset]\n",
    "        print('Dataset prepared')\n",
    "        print('Prepared dataset saved')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('Prepared dataset exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4ca49",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e855caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "sentiments = []\n",
    "str_sentiments = []\n",
    "for datum in dataset:\n",
    "    reviews.append(datum['review'])\n",
    "    sentiments.append(datum['sentiment'])\n",
    "    str_sentiments.append(num_to_sentiment(datum['sentiment']))\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    reviews,\n",
    "    sentiments,\n",
    "    test_size=conf('train-test-split.test-size'),\n",
    "    random_state=conf('train-test-split.random-stage'),\n",
    "    stratify=str_sentiments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c44599",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21d88e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing is done\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(\n",
    "    max_features=conf('vec.tfidf.max-features'),\n",
    "    norm=conf('vec.tfidf.norm'),\n",
    "    max_df=conf('vec.tfidf.max-df'),\n",
    "    min_df=conf('vec.tfidf.min-df'),\n",
    "    stop_words=conf('vec.tfidf.stop-words')\n",
    ")\n",
    "tf_idf_train_X = tf_idf_vectorizer.fit_transform(train_x)\n",
    "tf_idf_test_X = tf_idf_vectorizer.transform(test_x)\n",
    "print('Vectorizing is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402ac91",
   "metadata": {},
   "source": [
    "## Find optimal hyperparams with Gridsearch for TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "933445b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best hyperparameters are {'max_depth': None, 'n_estimators': 300}\n",
      "Best score is 0.7996250341236782\n"
     ]
    }
   ],
   "source": [
    "tf_idf_clf = RandomForestClassifier()\n",
    "tf_idf_grid = GridSearchCV(\n",
    "    tf_idf_clf,\n",
    "    param_grid=conf('hyper-params.param-grid'),\n",
    "    cv=conf('hyper-params.cv'),\n",
    "    scoring=conf('hyper-params.scoring'),\n",
    "    verbose=conf('hyper-params.verbose'),\n",
    "    n_jobs=conf('hyper-params.jobs')\n",
    ")\n",
    "tf_idf_model_grid = tf_idf_grid.fit(tf_idf_train_X, train_y)\n",
    "print('Best hyperparameters are ' + str(tf_idf_model_grid.best_params_))\n",
    "print('Best score is ' + str(tf_idf_model_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d98d3a",
   "metadata": {},
   "source": [
    "## Prediction for TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3642c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7992\n",
      "F1: 0.7991391873803713\n"
     ]
    }
   ],
   "source": [
    "tf_idf_clf.fit(tf_idf_train_X, train_y)\n",
    "prediction = tf_idf_clf.predict(tf_idf_test_X)\n",
    "\n",
    "tf_idf_accuracy_socre = accuracy_score(test_y, prediction)\n",
    "tf_idf_f1 = f1_score(test_y, prediction, average=\"macro\")\n",
    "print('Accuracy: ' + str(tf_idf_accuracy_socre))\n",
    "print('F1: ' +  str(tf_idf_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce27207",
   "metadata": {},
   "source": [
    "## word2Vec + TF-IDF vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "199ae776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing is done\n"
     ]
    }
   ],
   "source": [
    "keyed_vectors = gensim.models.KeyedVectors.load(conf('path.dataset.w2v-embedding'))\n",
    "\n",
    "class Vectors:\n",
    "    def __init__(self, wv):\n",
    "        self._wv = wv\n",
    "\n",
    "    def get(self, token: str):\n",
    "        return self._wv.vectors[self._wv.key_to_index[token]] if token in self._wv.key_to_index else None\n",
    "    \n",
    "vectors = Vectors(keyed_vectors)\n",
    "\n",
    "def vectorize(sentence: str):\n",
    "    weighs_data = tf_idf_vectorizer.transform([sentence]).tocoo()\n",
    "    vocab = tf_idf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    sentence_vector = []\n",
    "    for row, col, weight in zip(weighs_data.row, weighs_data.col, weighs_data.data):\n",
    "        token = vectors.get(vocab[col])\n",
    "        if token is not None:\n",
    "            sentence_vector.append(weight * token)\n",
    "    \n",
    "    if len(sentence_vector) == 0:\n",
    "        return None\n",
    "    return np.mean(sentence_vector, axis=0)\n",
    "\n",
    "def texts_to_vectors(texts: list[str], y: list[int]) -> tuple:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, text in enumerate(texts):\n",
    "        v = vectorize(text)\n",
    "        if v is not None:\n",
    "            X.append(v)\n",
    "            Y.append(y[index])\n",
    "    return X, Y, \n",
    "\n",
    "w2v_train_X, w2v_train_y = texts_to_vectors(train_x, train_y)\n",
    "w2v_test_X, w2v_test_y = texts_to_vectors(test_x, test_y)\n",
    "\n",
    "print('Vectorizing is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c29e2",
   "metadata": {},
   "source": [
    "## Find optimal hyperparams with Gridsearch for TF-IDF + Word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae75c3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best hyperparameters are {'max_depth': None, 'n_estimators': 200}\n",
      "Best score is 0.7724579309352221\n"
     ]
    }
   ],
   "source": [
    "w2v_clf = RandomForestClassifier()\n",
    "w2v_model_grid = GridSearchCV(\n",
    "    w2v_clf,\n",
    "    param_grid=conf('hyper-params.param-grid'),\n",
    "    cv=conf('hyper-params.cv'),\n",
    "    scoring=conf('hyper-params.scoring'),\n",
    "    verbose=conf('hyper-params.verbose'),\n",
    "    n_jobs=conf('hyper-params.jobs')\n",
    ")\n",
    "w2v_model_grid = w2v_model_grid.fit(w2v_train_X, w2v_train_y)\n",
    "print('Best hyperparameters are ' + str(w2v_model_grid.best_params_))\n",
    "print('Best score is ' + str(w2v_model_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80500d88",
   "metadata": {},
   "source": [
    "## Prediction for TF-IDF + Word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "085c6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7684\n",
      "F1: 0.7683955161371925\n"
     ]
    }
   ],
   "source": [
    "w2v_clf.fit(w2v_train_X, w2v_train_y)\n",
    "prediction = w2v_clf.predict(w2v_test_X)\n",
    "\n",
    "w2v_accuracy_socre = accuracy_score(w2v_test_y, prediction)\n",
    "w2v_f1 = f1_score(w2v_test_y, prediction, average=\"macro\")\n",
    "print('Accuracy: ' + str(accuracy_score(w2v_test_y, prediction)))\n",
    "print('F1: ' +  str(f1_score(w2v_test_y, prediction, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa782568",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "282827fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TF-IDF+Word2vec</td>\n",
       "      <td>0.799139</td>\n",
       "      <td>0.768396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['TF-IDF', tf_idf_accuracy_socre, w2v_accuracy_socre], ['TF-IDF+Word2vec', tf_idf_f1, w2v_f1]]\n",
    "df = pd.DataFrame(data, columns=['Type', 'Accuracy', 'F1'])\n",
    "\n",
    "HTML(df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b061e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
