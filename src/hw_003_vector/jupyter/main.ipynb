{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ff08f",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b525a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d519561",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284baf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'path.dataset.original': './IMDB Dataset.csv',\n",
    "            'path.dataset.prepared': './prepared IMDB Dataset.csv',\n",
    "            'path.dataset.w2v-embedding': './word2vec-google-news-300.model',\n",
    "\n",
    "            'train-test-split.test-size': 0.2,\n",
    "            'train-test-split.random-stage': 42,\n",
    "\n",
    "            'vec.tfidf.max-features': 300,\n",
    "            'vec.tfidf.norm': None,\n",
    "            'vec.tfidf.max-df': 0.95,\n",
    "            'vec.tfidf.min-df': 5,\n",
    "            'vec.tfidf.stop-words': 'english',\n",
    "\n",
    "            'hyper-params.on': False,\n",
    "            'hyper-params.cv': 3,\n",
    "            'hyper-params.scoring': 'accuracy',\n",
    "            'hyper-params.verbose': 3,\n",
    "            'hyper-params.jobs': -1,\n",
    "            'hyper-params.param-grid': {\n",
    "                'max_depth': [3, None],\n",
    "                'n_estimators': [10, 100, 200],\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args) == 0 or args[0] not in self._params:\n",
    "            return None\n",
    "        return self._params[args[0]]\n",
    "\n",
    "conf = Configurator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09733e",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e953b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_num(sentiment: str) -> int:\n",
    "    s = sentiment.lower()\n",
    "    if s == 'positive':\n",
    "        return 1\n",
    "    elif s == 'negative':\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "def num_to_sentiment(num: int) -> int:\n",
    "    return 'positive' if num == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6beb7",
   "metadata": {},
   "source": [
    "## Files checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e22f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(conf('path.dataset.original')):\n",
    "    print('Dataset is absent: ' + conf('path.dataset.original'))\n",
    "    \n",
    "embedding_path = conf('path.dataset.w2v-embedding')\n",
    "if not os.path.isfile(embedding_path):\n",
    "    print('Word2vec embedding is absent: ' + embedding_path + '; embedding source: https://huggingface.co/fse/word2vec-google-news-300/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e42b7",
   "metadata": {},
   "source": [
    "## Reset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "raw_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8eafe3",
   "metadata": {},
   "source": [
    "## Load preapared dataset if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b8dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(conf('path.dataset.prepared'), encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        dataset = [{'review': item['review'], 'sentiment': int(item['sentiment'])} for item in reader]\n",
    "        print('Prepared dataset loaded')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015b2bd",
   "metadata": {},
   "source": [
    "## Load and prepare dataset if prepared does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e1d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset exists\n"
     ]
    }
   ],
   "source": [
    "if dataset is None:\n",
    "    \n",
    "    tag_filter = re.compile(r'</?[a-z][\\w=\" -]*/?>')\n",
    "    punctuation_filter = re.compile(r'[.,!?*_)(]+')\n",
    "    space_filter = re.compile(r'\\s+')\n",
    "    \n",
    "    def filter_text(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = tag_filter.sub(' ', text)\n",
    "        text = punctuation_filter.sub(' ', text)\n",
    "        text = space_filter.sub(' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        with open(conf('path.dataset.original'), encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            raw_dataset = [item for item in reader]\n",
    "        dataset = []\n",
    "        print('Dataset loaded')\n",
    "\n",
    "        for datum in raw_dataset:\n",
    "            num_sentiment = sentiment_to_num(datum['sentiment'])\n",
    "            if num_sentiment != -1:\n",
    "                dataset.append({\n",
    "                    'review': filter_text(datum['review']),\n",
    "                    'sentiment': num_sentiment\n",
    "                })\n",
    "\n",
    "        with open(conf('path.dataset.prepared'), 'w', encoding='utf-8') as file:\n",
    "            fieldnames = ['review', 'sentiment']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            [writer.writerow(datum) for datum in dataset]\n",
    "        print('Dataset prepared')\n",
    "        print('Prepared dataset saved')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('Prepared dataset exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4ca49",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e855caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "sentiments = []\n",
    "str_sentiments = []\n",
    "for datum in dataset:\n",
    "    reviews.append(datum['review'])\n",
    "    sentiments.append(datum['sentiment'])\n",
    "    str_sentiments.append(num_to_sentiment(datum['sentiment']))\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    reviews,\n",
    "    sentiments,\n",
    "    test_size=conf('train-test-split.test-size'),\n",
    "    random_state=conf('train-test-split.random-stage'),\n",
    "    stratify=str_sentiments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c44599",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d88e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(\n",
    "    max_features=conf('vec.tfidf.max-features'),\n",
    "    norm=conf('vec.tfidf.norm'),\n",
    "    max_df=conf('vec.tfidf.max-df'),\n",
    "    min_df=conf('vec.tfidf.min-df'),\n",
    "    stop_words=conf('vec.tfidf.stop-words')\n",
    ")\n",
    "tf_idf_train_X = tf_idf_vectorizer.fit_transform(train_x)\n",
    "tf_idf_test_X = tf_idf_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402ac91",
   "metadata": {},
   "source": [
    "## Find optimal hyperparams with Gridsearch for TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933445b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best hyperparameters are {'max_depth': None, 'n_estimators': 200}\n",
      "Best score is 0.7978500084971002\n"
     ]
    }
   ],
   "source": [
    "tf_idf_clf = RandomForestClassifier()\n",
    "tf_idf_grid = GridSearchCV(\n",
    "    tf_idf_clf,\n",
    "    param_grid=conf('hyper-params.param-grid'),\n",
    "    cv=conf('hyper-params.cv'),\n",
    "    scoring=conf('hyper-params.scoring'),\n",
    "    verbose=conf('hyper-params.verbose'),\n",
    "    n_jobs=conf('hyper-params.jobs')\n",
    ")\n",
    "tf_idf_model_grid = tf_idf_grid.fit(tf_idf_train_X, train_y)\n",
    "print('Best hyperparameters are ' + str(tf_idf_model_grid.best_params_))\n",
    "print('Best score is ' + str(tf_idf_model_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d98d3a",
   "metadata": {},
   "source": [
    "## Prediction for TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3642c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7981\n",
      "F1: 0.7980527261626673\n"
     ]
    }
   ],
   "source": [
    "tf_idf_clf.fit(tf_idf_train_X, train_y)\n",
    "prediction = tf_idf_clf.predict(tf_idf_test_X)\n",
    "\n",
    "print('Accuracy: ' + str(accuracy_score(test_y, prediction)))\n",
    "print('F1: ' +  str(f1_score(test_y, prediction, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce27207",
   "metadata": {},
   "source": [
    "## word2Vec + TF-IDF vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "199ae776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34937796  0.17211932  0.14341433  0.41276607 -0.08646991 -0.13028307\n",
      "  0.21137387 -0.25993258  0.32833505  0.24431635 -0.1404701  -0.39253506\n",
      " -0.2141691   0.08864664 -0.42881262  0.4092807   0.32810846  0.75480205\n",
      " -0.03262123 -0.29178265  0.0275266   0.4046886   0.22331345  0.1317039\n",
      "  0.36825624 -0.4561386  -0.2252761   0.40394044  0.27249417 -0.05284693\n",
      " -0.5433183   0.09425654 -0.24595608  0.22773342  0.39467254 -0.14111935\n",
      "  0.34730637  0.26775655  0.10978246  0.3793744   0.36419073 -0.39584658\n",
      "  0.53749305  0.1331056  -0.02636757 -0.00756086 -0.2753962  -0.19376205\n",
      "  0.17768179  0.19817871 -0.3671048   0.09754681 -0.118329    0.09711701\n",
      "  0.16718929  0.00847303  0.11380235 -0.2665592   0.17597872 -0.2612956\n",
      " -0.15350886  0.31791896 -0.30344462 -0.17439282 -0.16186476  0.13511072\n",
      " -0.26034293  0.22596313 -0.02519921  0.22305809  0.08243472  0.09376258\n",
      "  0.24954689  0.02308584 -0.6920633  -0.02220535  0.10747418  0.35778365\n",
      "  0.3853655   0.38646713  0.05309062 -0.01224812  0.16668852  0.05121\n",
      " -0.24211057 -0.13411753 -0.43450496  0.4775497   0.38156965  0.15210514\n",
      " -0.05102795  0.19209631 -0.4018915  -0.07746161 -0.2676227  -0.3136473\n",
      "  0.05055765  0.41009486 -0.09203853 -0.16568024 -0.3478439  -0.05617229\n",
      " -0.00174863  0.01613653 -0.17217606  0.11549126  0.05559558 -0.26365972\n",
      "  0.07055443 -0.2971522  -0.384248   -0.04113753  0.04845257 -0.15813817\n",
      "  0.4373211  -0.01967283  0.19542111 -0.19252907  0.5221735  -0.11809447\n",
      " -0.37950674  0.08600172 -0.17139776  0.33124018 -0.15850653 -0.11207402\n",
      " -0.07474256 -0.04087859 -0.02631089  0.17132199 -0.53048193 -0.47182932\n",
      " -0.34391963  0.15997095 -0.12116968  0.09163622  0.34362116  0.15643431\n",
      " -0.10325403 -0.07750397  0.16594404 -0.20410036  0.2164572   0.01962543\n",
      " -0.0383784   0.22162639 -0.09315986 -0.3610605  -0.04734887  0.04253396\n",
      "  0.25878844  0.38015747 -0.30920735  0.3597584  -0.08406747 -0.11505387\n",
      " -0.13764648 -0.48548985 -0.18043268 -0.07758962 -0.11545312  0.56261843\n",
      "  0.30786577  0.33540046  0.03285681 -0.45644596  0.33665395 -0.2418164\n",
      "  0.29293376 -0.03678973 -0.3236963   0.00420812  0.11621454 -0.00276237\n",
      " -0.14768757  0.00584198  0.3497877  -0.47146216 -0.08322767  0.2010902\n",
      " -0.10687784 -0.20139022  0.38705146  0.14467585 -0.05116649  0.04902508\n",
      " -0.3204641  -0.09614358  0.18726712 -0.18631601  0.1769644  -0.1123674\n",
      "  0.26872304  0.1548814   0.01101508  0.29235473 -0.1788311  -0.01565117\n",
      " -0.2793052  -0.24182479  0.19124965  0.09660744 -0.33325326  0.03518039\n",
      "  0.4357524  -0.29003483 -0.18031095  0.02222874 -0.2516135   0.0926716\n",
      "  0.02396184  0.5271541   0.16340916 -0.02735113 -0.48462725 -0.126561\n",
      "  0.2379566  -0.07604915 -0.4278295  -0.0724016  -0.15132047  0.2921373\n",
      "  0.01582496  0.13384746  0.27268097 -0.10335163  0.07045374 -0.0959242\n",
      " -0.02643048 -0.00337758  0.05042461 -0.09430788 -0.09842525  0.17955554\n",
      "  0.49950492  0.06766187 -0.14173353 -0.28248796  0.51451844  0.27316675\n",
      "  0.40805015  0.3621585  -0.15211451 -0.32146874 -0.12337779  0.29711506\n",
      "  0.11839514  0.15854917  0.14985359 -0.23132123 -0.10143805  0.07676836\n",
      "  0.17889096  0.4481356   0.14916028  0.05139354 -0.07289501 -0.13868992\n",
      " -0.3696914  -0.413427   -0.4218087   0.06224128 -0.0985982   0.04599119\n",
      " -0.00084366  0.77118665  0.01299157 -0.4203919  -0.3838289   0.06571616\n",
      "  0.01618179  0.36498323  0.43891838  0.04203278  0.39936045 -0.1703353\n",
      " -0.14476714 -0.2674649  -0.29677317  0.03539294  0.125831   -0.21817385\n",
      " -0.09107167  0.3029614   0.09993132 -0.18330908 -0.36256215 -0.25576475\n",
      " -0.04447958  0.03969507 -0.38780865  0.37399408 -0.35948038  0.228191\n",
      " -0.36418167  0.10613453  0.38805923 -0.21276093  0.08777034  0.01733019]\n",
      "[ 1.73877344e-01  1.58944145e-01  1.41069487e-01  4.28454697e-01\n",
      " -2.00171724e-01  7.89601065e-04  3.13443482e-01 -4.79262531e-01\n",
      "  3.28342378e-01  2.00932115e-01  1.23474553e-01 -5.81895649e-01\n",
      " -4.83072810e-02 -1.07195685e-02 -3.53891730e-01  2.83341914e-01\n",
      "  2.59604484e-01  2.80252635e-01  3.61618474e-02 -1.94342077e-01\n",
      " -3.00967731e-02  1.86524212e-01  2.07337350e-01  1.15738034e-01\n",
      "  1.04946546e-01  1.67430528e-02 -2.60982603e-01  1.63536042e-01\n",
      "  6.83349743e-02  5.16167283e-02 -2.36746579e-01  4.75025699e-02\n",
      " -1.25164136e-01  8.16517994e-02  2.58105695e-01  4.03279103e-02\n",
      "  2.44012684e-01 -8.53925571e-02  2.26454008e-02  3.42762589e-01\n",
      "  2.96196580e-01 -1.30163848e-01  3.46698165e-01 -1.80915117e-01\n",
      " -1.40006185e-01  6.33280575e-02 -1.22424759e-01  7.41146952e-02\n",
      "  1.58469930e-01  5.02469316e-02 -1.42897606e-01  4.06727940e-02\n",
      " -5.91919087e-02 -1.18421115e-01  1.31122932e-01 -5.18120155e-02\n",
      " -1.35333285e-01 -1.06022447e-01  1.81598648e-01 -2.06712827e-01\n",
      "  4.76152077e-02  4.65467930e-01 -1.79248512e-01 -1.73060179e-01\n",
      "  2.83223614e-02  1.40900910e-01 -1.73108444e-01  2.17722148e-01\n",
      " -1.62239686e-01  9.37607735e-02 -4.67079543e-02  1.09352268e-01\n",
      "  2.76623219e-01 -3.99582461e-02 -4.84302878e-01 -3.53176266e-01\n",
      "  1.48341060e-01  9.37840939e-02  2.58354455e-01  3.77931923e-01\n",
      "  8.89738351e-02 -3.33125032e-02  3.21041286e-01 -2.67049372e-01\n",
      " -2.71566331e-01 -2.97960602e-02 -3.68504494e-01  3.88547599e-01\n",
      "  2.09041283e-01  1.62064657e-01  7.13283848e-03  1.25570014e-01\n",
      " -1.04332961e-01 -2.03374550e-01 -8.31741393e-02 -2.52116323e-01\n",
      "  5.74606322e-02  1.12910338e-01  3.02316863e-02 -1.44909635e-01\n",
      " -9.35030431e-02  1.71022061e-02  5.45101427e-02  6.47367164e-02\n",
      " -1.94106728e-01 -7.47437179e-02 -4.01656292e-02 -1.71241820e-01\n",
      "  2.35247817e-02 -1.94363460e-01 -1.40338734e-01 -1.20072089e-01\n",
      "  1.83334410e-01  5.49809523e-02  4.49753553e-01 -1.38341323e-01\n",
      "  5.57377376e-02 -7.83005506e-02  2.22910255e-01  1.11316532e-01\n",
      " -3.72112453e-01 -1.55939281e-01 -1.52845830e-01  1.76013917e-01\n",
      " -2.69944500e-02 -2.17732668e-01 -1.14384405e-01 -1.78051919e-01\n",
      " -1.78272337e-01  2.26002365e-01 -2.80659169e-01 -3.53927195e-01\n",
      " -2.08337441e-01  8.46378654e-02 -5.34651354e-02 -3.22665483e-01\n",
      "  1.46443829e-01  9.37420800e-02 -4.69648503e-02 -5.70790144e-04\n",
      "  2.55900830e-01 -3.90784234e-01  1.25095621e-01  1.24868639e-02\n",
      "  6.19948469e-02  1.21209316e-01 -4.14043851e-02 -7.71042854e-02\n",
      " -1.21918365e-01 -1.05086014e-01  1.66599512e-01  2.88259625e-01\n",
      " -3.39361399e-01  1.48922741e-01 -2.05656499e-01 -3.91092524e-02\n",
      " -2.41500586e-01 -3.67073417e-01 -2.39328086e-01 -1.88816473e-01\n",
      " -3.48766893e-02  3.20302784e-01  1.85831919e-01  1.66349381e-01\n",
      "  3.16484831e-02 -1.19556099e-01  1.73352659e-01 -1.81546271e-01\n",
      "  2.77222186e-01  2.71533895e-02 -2.10523099e-01 -1.08426720e-01\n",
      "  1.16374262e-01 -3.70204777e-01 -2.87558027e-02 -4.56602778e-03\n",
      "  2.72900909e-01 -1.99241892e-01 -6.10646270e-02  1.41980872e-01\n",
      " -1.45264491e-01 -1.84830889e-01  2.93847740e-01 -9.74877253e-02\n",
      " -1.97020397e-01 -1.23390421e-01 -2.54607022e-01  8.62134844e-02\n",
      "  3.01984131e-01 -2.82988772e-02  2.90062606e-01 -5.93755431e-02\n",
      "  1.93068027e-01 -1.26855612e-01 -9.18346345e-02  9.52886045e-02\n",
      " -6.08346201e-02 -1.54612027e-02 -3.24223816e-01 -3.47063124e-01\n",
      "  7.55156800e-02  1.97425619e-01 -3.26499313e-01  1.40055835e-01\n",
      "  2.49215931e-01 -4.43152972e-02 -2.89642125e-01  4.32762355e-02\n",
      " -1.22953542e-01 -1.37455374e-01 -1.11655861e-01  5.81682205e-01\n",
      " -2.14621678e-01  4.91935536e-02 -4.55432475e-01 -8.02054331e-02\n",
      "  3.57662141e-01 -3.25334482e-02 -2.64073282e-01 -1.55840874e-01\n",
      " -2.02896316e-02  4.16488312e-02 -7.64679611e-02 -2.57210829e-03\n",
      "  6.84699342e-02 -3.85997891e-02  1.73342675e-01  8.41155723e-02\n",
      " -2.68908478e-02  3.20189185e-02  1.57178938e-01 -1.92087486e-01\n",
      "  1.26548344e-02 -1.30615339e-01  2.53876388e-01  8.98691416e-02\n",
      " -1.61096677e-01 -7.72697777e-02  3.68740559e-01 -4.06595208e-02\n",
      "  2.65807927e-01  3.80012356e-02  1.41355589e-01 -4.39762771e-01\n",
      " -2.11266652e-01  5.77639937e-02  1.68215573e-01  1.34676754e-01\n",
      " -9.15529281e-02 -1.62754625e-01  5.63288853e-02  2.83638120e-01\n",
      "  3.87226999e-01  2.59546280e-01 -4.09009419e-02 -1.21549256e-01\n",
      "  9.14692006e-04 -2.47666938e-03 -2.38499820e-01 -4.28715020e-01\n",
      " -3.80827248e-01 -1.40127793e-01 -2.25945525e-02  6.56320229e-02\n",
      "  4.87749502e-02  3.90568703e-01  1.12885818e-01 -8.86817724e-02\n",
      " -2.91995317e-01 -2.32358482e-02  3.06893457e-02  2.51713574e-01\n",
      "  2.55543917e-01  1.83430120e-01  2.77325362e-01 -4.05741900e-01\n",
      " -2.96065539e-01 -3.39786410e-01 -1.17349222e-01 -9.74785984e-02\n",
      "  1.36814699e-01 -9.12536457e-02 -1.64132848e-01  2.48525620e-01\n",
      " -8.50025043e-02 -1.51730627e-01 -3.28155577e-01 -1.04441963e-01\n",
      "  2.37030592e-02  1.94122255e-01 -3.76557499e-01  9.68166664e-02\n",
      " -4.65428621e-01 -5.42046055e-02 -3.18303049e-01 -2.02381626e-01\n",
      " -1.07887752e-01 -6.13443218e-02  1.06766395e-01 -1.46332636e-01]\n",
      "39997\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "keyed_vectors = gensim.models.KeyedVectors.load(conf('path.dataset.w2v-embedding'))\n",
    "# x = gensim.models.Word2Vec.load(conf('path.dataset.w2v-embedding'))\n",
    "# print(type(keyed_vectors))\n",
    "\n",
    "class Vectors:\n",
    "    def __init__(self, wv):\n",
    "        self._wv = wv\n",
    "\n",
    "    def get(self, token: str):\n",
    "        return self._wv.vectors[self._wv.key_to_index[token]] if token in self._wv.key_to_index else None\n",
    "    \n",
    "vectors = Vectors(keyed_vectors)\n",
    "\n",
    "def vectorize(sentence: str):\n",
    "    weighs_data = tf_idf_vectorizer.transform([sentence]).tocoo()\n",
    "    vocab = tf_idf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    sentence_vector = []\n",
    "    for row, col, weight in zip(weighs_data.row, weighs_data.col, weighs_data.data):\n",
    "        token = vectors.get(vocab[col])\n",
    "        if token is not None:\n",
    "            sentence_vector.append(weight * token)\n",
    "    \n",
    "    if len(sentence_vector) == 0:\n",
    "        return None\n",
    "    return np.mean(sentence_vector, axis=0)\n",
    "\n",
    "w2v_train_X = []\n",
    "for text in train_x:\n",
    "    v = vectorize(text)\n",
    "    if v is not None:\n",
    "        w2v_train_X.append(v)\n",
    "\n",
    "w2v_test_X = []\n",
    "for text in test_x:\n",
    "    v = vectorize(text)\n",
    "    if v is not None:\n",
    "        w2v_test_X.append(v)\n",
    "\n",
    "\n",
    "print(w2v_train_X[0])\n",
    "print(w2v_test_X[0])\n",
    "print(len(w2v_train_X))\n",
    "print(len(w2v_test_X))\n",
    "\n",
    "# x = \"\"\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn't mess around the first episode i ever saw struck me as so nasty it was surreal i couldn't say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards who'll be sold out for a nickel inmates who'll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side\"\"\"\n",
    "# v = vectorize(x)\n",
    "\n",
    "\n",
    "# tf_idf_train_X = tf_idf_vectorizer.fit_transform(train_x)\n",
    "# tf_idf_test_X = tf_idf_vectorizer.transform(test_x)\n",
    "\n",
    "\n",
    "# def tfidf_embedding(vectors: Vectors, sentence: str, vectorizer):\n",
    "#     weighs_data1 = vectorizer.transform([sentence])\n",
    "#     weighs_data = vectorizer.transform([sentence]).tocoo()\n",
    "\n",
    "#     vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "#     sentence_vector = []\n",
    "#     for row, col, weight in zip(weighs_data.row, weighs_data.col, weighs_data.data):\n",
    "#         print(row, col, weight)\n",
    "#         print(vocab[col])\n",
    "#         token = vectors.get(vocab[col])\n",
    "#         if token is not None:\n",
    "#             sentence_vector.append(weight * token)\n",
    "#     v = np.mean(sentence_vector, axis=0)\n",
    "#     print(v.shape)\n",
    "#     print(v)\n",
    "\n",
    "#     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
