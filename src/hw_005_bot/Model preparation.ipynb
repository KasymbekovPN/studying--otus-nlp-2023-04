{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b952b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc22f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf3189",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a2f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants are initialized.\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "TRAIN_KEY = 'train'\n",
    "TEST_KEY = 'test'\n",
    "VAL_KEY = 'val'\n",
    "\n",
    "DATA_DIRECTORY = '.\\\\data'\n",
    "DANETQA_INPUT = os.path.join(DATA_DIRECTORY, 'danetqa_paths.json')\n",
    "\n",
    "PREPARED_BERT_PATH = 'ai-forever/ruBert-base'\n",
    "PREPARED_T5_BASE_PATH = 'ai-forever/ruT5-base'\n",
    "\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "TRAIN_FRAC = 0.9\n",
    "VAL_FRAC = 0.9\n",
    "# BATCH_SIZE = 16\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "SENTENCE_COLS = ['question', 'passage']\n",
    "USE_RAW_MAX_LENGTH = True\n",
    "\n",
    "print('Constants are initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bab601",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED) # check needness\n",
    "np.random.seed(RANDOM_SEED) # check needness\n",
    "torch.manual_seed(RANDOM_SEED) # check needness\n",
    "torch.cuda.manual_seed(RANDOM_SEED) # check needness\n",
    "\n",
    "print('Random seed is set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131a912",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439aad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Device is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74fa9",
   "metadata": {},
   "source": [
    "## Model & tokenizer & optimizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7614d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, optimizer are created.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(PREPARED_T5_BASE_PATH, use_fast=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(PREPARED_T5_BASE_PATH)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "\n",
    "print('Model, tokenizer, optimizer are created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bc0d0",
   "metadata": {},
   "source": [
    "## Loading & preparation of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f71d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Path</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>.\\data\\DaNetQA\\train.jsonl</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>.\\data\\DaNetQA\\test.jsonl</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val</td>\n",
       "      <td>.\\data\\DaNetQA\\val.jsonl</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & preparation of dataframes are done.\n"
     ]
    }
   ],
   "source": [
    "paths_df = pd.read_json(DANETQA_INPUT, lines=True)\n",
    "\n",
    "train_file_path = paths_df.get(TRAIN_KEY).values[0]\n",
    "test_file_path = paths_df.get(TEST_KEY).values[0]\n",
    "val_file_path = paths_df.get(VAL_KEY).values[0]\n",
    "\n",
    "original_train_df = pd.read_json(train_file_path, lines=True)\n",
    "original_val_df = pd.read_json(val_file_path, lines=True)\n",
    "original_test_df = pd.read_json(test_file_path, lines=True)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, train_file_path, len(original_train_df)],\n",
    "        [TEST_KEY, test_file_path, len(original_val_df)],\n",
    "        [VAL_KEY, val_file_path, len(original_test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Path', 'Size']\n",
    ")\n",
    "print('Original datasets:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "def enrich_df(df) -> None:\n",
    "    df['sentence'] = df[SENTENCE_COLS].apply(lambda row: ''.join(row).lower(), axis=1)\n",
    "    df['l'] = df[['label']].apply(lambda label: 1 if label[0] else 0, axis=1)\n",
    "\n",
    "random_index = original_train_df.sample(frac=TRAIN_FRAC, random_state=RANDOM_SEED).index\n",
    "test_df0 = original_train_df[~original_train_df.index.isin(random_index)]\n",
    "train_df = original_train_df[original_train_df.index.isin(random_index)].reset_index(drop=True)\n",
    "\n",
    "random_index = original_val_df.sample(frac=VAL_FRAC, random_state=RANDOM_SEED).index\n",
    "test_df1 = original_val_df[~original_val_df.index.isin(random_index)]\n",
    "val_df = original_val_df[original_val_df.index.isin(random_index)].reset_index(drop=True)\n",
    "\n",
    "test_df = pd.concat([test_df0, test_df1], ignore_index=True)\n",
    "\n",
    "enrich_df(train_df)\n",
    "enrich_df(val_df)\n",
    "enrich_df(test_df)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, len(train_df)],\n",
    "        [TEST_KEY, len(val_df)],\n",
    "        [VAL_KEY, len(test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Size']\n",
    ")\n",
    "print('Used datasets:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "print('Loading & preparation of dataframes are done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70889b",
   "metadata": {},
   "source": [
    "## Define max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabc41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of max_length:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Raw</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Binary-based</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Used</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sentences is defined.\n"
     ]
    }
   ],
   "source": [
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_df.sentence.values, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(val_df.sentence.values, raw_max_length)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_df.sentence.values, raw_max_length)\n",
    "\n",
    "binary_based_max_length = define_max_length(raw_max_length, 1)\n",
    "max_length = raw_max_length if USE_RAW_MAX_LENGTH else binary_based_max_length\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        ['Raw', raw_max_length],\n",
    "        ['Binary-based', binary_based_max_length],\n",
    "        ['Used', max_length]\n",
    "    ],\n",
    "    columns=['Type', 'Size']\n",
    ")\n",
    "print('Definition of max_length:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "print('Maximum length of sentences is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de36a4",
   "metadata": {},
   "source": [
    "## Datasets & dataloaders creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb63c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets & dataloaders creation is done\n"
     ]
    }
   ],
   "source": [
    "class T5TestDataset(TorchDataset):\n",
    "    def __init__(self, text, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._text.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item])\n",
    "        return {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=self._length)\n",
    "\n",
    "\n",
    "class T5TrainDataset(TorchDataset):\n",
    "    POS_LABEL = 'верно'\n",
    "    NEG_LABEL = 'неверно'\n",
    "\n",
    "    def __init__(self, text, label, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._label = label.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._label.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item], self._length)\n",
    "        output = {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "        label = self.POS_LABEL if self._label[item] == 1 else self.NEG_LABEL\n",
    "        label = self._tokenize(label, length=2).input_ids.reshape(-1).to(self._device)\n",
    "\n",
    "        output.update({'labels': label})\n",
    "        return output\n",
    "\n",
    "    def _tokenize(self, text, length):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=length)\n",
    "\n",
    "    \n",
    "train_dataset = T5TrainDataset(train_df['sentence'], train_df['l'], tokenizer, max_length, device)\n",
    "val_dataset = T5TrainDataset(val_df['sentence'], val_df['l'], tokenizer, max_length, device)\n",
    "test_dataset = T5TestDataset(test_df['sentence'], tokenizer, max_length, device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('Datasets & dataloaders creation is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb597a8",
   "metadata": {},
   "source": [
    "## Scheduler creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193bfca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler is created\n"
     ]
    }
   ],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * EPOCHS\n",
    ")\n",
    "\n",
    "print('Scheduler is created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa2ee0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader len: 197\n",
      "\n",
      "======= Epoch 1 / 1 =======\n",
      "\n",
      "Training...\n",
      "\n",
      "0 2023-08-20 17:14:04.336524\n",
      "1 2023-08-20 17:14:37.974944\n",
      "2 2023-08-20 17:15:10.658004\n",
      "3 2023-08-20 17:15:43.804539\n",
      "4 2023-08-20 17:16:18.112430\n",
      "5 2023-08-20 17:16:50.570007\n",
      "6 2023-08-20 17:17:23.995431\n",
      "7 2023-08-20 17:17:56.435516\n",
      "8 2023-08-20 17:18:29.899453\n",
      "9 2023-08-20 17:19:02.346536\n",
      "\t\tBatch 10 of 197, loss : 7.695\n",
      "10 2023-08-20 17:19:35.819505\n",
      "11 2023-08-20 17:20:08.259295\n",
      "12 2023-08-20 17:20:41.686737\n",
      "13 2023-08-20 17:21:14.102651\n",
      "14 2023-08-20 17:21:47.547972\n",
      "15 2023-08-20 17:22:20.013483\n",
      "16 2023-08-20 17:22:53.441986\n",
      "17 2023-08-20 17:23:25.874492\n",
      "18 2023-08-20 17:23:59.306712\n",
      "19 2023-08-20 17:24:31.740938\n",
      "\t\tBatch 20 of 197, loss : 2.888\n",
      "20 2023-08-20 17:25:05.303006\n",
      "21 2023-08-20 17:25:37.799561\n",
      "22 2023-08-20 17:26:11.187247\n",
      "23 2023-08-20 17:26:43.683505\n",
      "24 2023-08-20 17:27:17.126828\n",
      "25 2023-08-20 17:27:49.575983\n",
      "26 2023-08-20 17:28:23.003659\n",
      "27 2023-08-20 17:28:55.484950\n",
      "28 2023-08-20 17:29:29.128305\n",
      "29 2023-08-20 17:30:01.584497\n",
      "\t\tBatch 30 of 197, loss : 2.737\n",
      "30 2023-08-20 17:30:35.022353\n",
      "31 2023-08-20 17:31:07.484323\n",
      "32 2023-08-20 17:31:40.936799\n",
      "33 2023-08-20 17:32:13.382824\n",
      "34 2023-08-20 17:32:46.792534\n",
      "35 2023-08-20 17:33:19.236533\n",
      "36 2023-08-20 17:33:52.658860\n",
      "37 2023-08-20 17:34:25.095381\n",
      "38 2023-08-20 17:34:58.521798\n",
      "39 2023-08-20 17:35:31.021833\n",
      "\t\tBatch 40 of 197, loss : 0.939\n",
      "40 2023-08-20 17:36:04.481224\n",
      "41 2023-08-20 17:36:36.932839\n",
      "42 2023-08-20 17:37:10.428579\n",
      "43 2023-08-20 17:37:42.945306\n",
      "44 2023-08-20 17:38:16.513025\n",
      "45 2023-08-20 17:38:49.020703\n",
      "46 2023-08-20 17:39:22.449667\n",
      "47 2023-08-20 17:39:54.966065\n",
      "48 2023-08-20 17:40:28.516999\n",
      "49 2023-08-20 17:41:00.968772\n",
      "\t\tBatch 50 of 197, loss : 0.695\n",
      "50 2023-08-20 17:41:34.415089\n",
      "51 2023-08-20 17:42:06.898780\n",
      "52 2023-08-20 17:42:40.341032\n",
      "53 2023-08-20 17:43:12.802861\n",
      "54 2023-08-20 17:43:46.295734\n",
      "55 2023-08-20 17:44:18.831733\n",
      "56 2023-08-20 17:44:52.345499\n",
      "57 2023-08-20 17:45:24.841453\n",
      "58 2023-08-20 17:45:58.261955\n",
      "59 2023-08-20 17:46:30.759742\n",
      "\t\tBatch 60 of 197, loss : 0.608\n",
      "60 2023-08-20 17:47:04.221285\n",
      "61 2023-08-20 17:47:36.699400\n",
      "62 2023-08-20 17:48:10.147417\n",
      "63 2023-08-20 17:48:42.625647\n",
      "64 2023-08-20 17:49:16.118695\n",
      "65 2023-08-20 17:49:48.563409\n",
      "66 2023-08-20 17:50:22.082004\n",
      "67 2023-08-20 17:50:54.524166\n",
      "68 2023-08-20 17:51:28.002125\n",
      "69 2023-08-20 17:52:00.505888\n",
      "\t\tBatch 70 of 197, loss : 0.393\n",
      "70 2023-08-20 17:52:34.007507\n",
      "71 2023-08-20 17:53:06.505059\n",
      "72 2023-08-20 17:53:40.013145\n",
      "73 2023-08-20 17:54:12.507123\n",
      "74 2023-08-20 17:54:45.949444\n",
      "75 2023-08-20 17:55:18.366335\n",
      "76 2023-08-20 17:55:51.855982\n",
      "77 2023-08-20 17:56:24.406107\n",
      "78 2023-08-20 17:56:57.877390\n",
      "79 2023-08-20 17:57:30.334583\n",
      "\t\tBatch 80 of 197, loss : 0.531\n",
      "80 2023-08-20 17:58:03.784395\n",
      "81 2023-08-20 17:58:36.235858\n",
      "82 2023-08-20 17:59:09.682786\n",
      "83 2023-08-20 17:59:42.200898\n",
      "84 2023-08-20 18:00:15.636231\n",
      "85 2023-08-20 18:00:48.073579\n",
      "86 2023-08-20 18:01:21.492236\n",
      "87 2023-08-20 18:01:53.956008\n",
      "88 2023-08-20 18:02:27.390019\n",
      "89 2023-08-20 18:02:59.919175\n",
      "\t\tBatch 90 of 197, loss : 0.779\n",
      "90 2023-08-20 18:03:33.440444\n",
      "91 2023-08-20 18:04:05.895779\n",
      "92 2023-08-20 18:04:39.420798\n",
      "93 2023-08-20 18:05:11.957091\n",
      "94 2023-08-20 18:05:45.545944\n",
      "95 2023-08-20 18:06:18.077732\n",
      "96 2023-08-20 18:06:51.521955\n",
      "97 2023-08-20 18:07:23.947690\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "# n_epochs = conf('train.epochs.t5')\n",
    "\n",
    "dl_length = len(train_dataloader)\n",
    "\n",
    "# <\n",
    "print(f'dataloader len: {dl_length}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n======= Epoch {epoch + 1} / {EPOCHS} =======\\n')\n",
    "\n",
    "    print('Training...\\n')\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # < del\n",
    "        print(f'{batch_id} {datetime.datetime.now()}')\n",
    "        if (((batch_id + 1) % 10 == 0) and not batch_id == 0) or (batch_id == dl_length - 1):\n",
    "            print(f'\\t\\tBatch {batch_id+1} of {dl_length}, loss : {loss.item():.3f}')\n",
    "        \n",
    "    print('Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loss = [model(**batch).loss.item() for batch in eval_dataloader]\n",
    "        \n",
    "    print(f'\\tValidation loss: {np.sum(eval_loss)/len(eval_loss)}')\n",
    "    \n",
    "print('Trainig complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c679d5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2bef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## test\n",
    "\n",
    "# test_acceptable = test_dataframe.acceptable\n",
    "# print(f'Positive samples: {test_acceptable.sum()} of {len(test_acceptable)} ({100.0*test_acceptable.sum()/len(test_acceptable):.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560b557",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bc890d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71684227",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
