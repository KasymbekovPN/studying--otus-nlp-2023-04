{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b952b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afc22f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf3189",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8a2f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants are initialized.\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "TRAIN_KEY = 'train'\n",
    "TEST_KEY = 'test'\n",
    "VAL_KEY = 'val'\n",
    "\n",
    "DATA_DIRECTORY = '.\\\\data'\n",
    "DANETQA_INPUT = os.path.join(DATA_DIRECTORY, 'danetqa_paths.json')\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join(DATA_DIRECTORY, 'saved')\n",
    "\n",
    "PREPARED_BERT_PATH = 'ai-forever/ruBert-base'\n",
    "PREPARED_T5_BASE_PATH = 'ai-forever/ruT5-base'\n",
    "\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "TRAIN_FRAC = 0.9\n",
    "VAL_FRAC = 0.9\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "SENTENCE_COLS = ['question', 'passage']\n",
    "\n",
    "USE_RAW_MAX_LENGTH = True\n",
    "USE_BY_THRESHOLD = False\n",
    "TOKEN_SIZE_THRESHOLD = 300\n",
    "\n",
    "TESTING_FROM_FILES = True\n",
    "\n",
    "print('Constants are initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bab601",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ec1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED) # check needness\n",
    "np.random.seed(RANDOM_SEED) # check needness\n",
    "torch.manual_seed(RANDOM_SEED) # check needness\n",
    "torch.cuda.manual_seed(RANDOM_SEED) # check needness\n",
    "\n",
    "print('Random seed is set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131a912",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "439aad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Device is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74fa9",
   "metadata": {},
   "source": [
    "## Model & tokenizer & optimizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e7614d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, optimizer are created.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(PREPARED_T5_BASE_PATH, use_fast=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(PREPARED_T5_BASE_PATH)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "\n",
    "print('Model, tokenizer, optimizer are created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bc0d0",
   "metadata": {},
   "source": [
    "## Loading & preparation of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f71d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Path</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>.\\data\\DaNetQA\\train.jsonl</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>.\\data\\DaNetQA\\test.jsonl</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val</td>\n",
       "      <td>.\\data\\DaNetQA\\val.jsonl</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & preparation of dataframes are done.\n"
     ]
    }
   ],
   "source": [
    "paths_df = pd.read_json(DANETQA_INPUT, lines=True)\n",
    "\n",
    "train_file_path = paths_df.get(TRAIN_KEY).values[0]\n",
    "test_file_path = paths_df.get(TEST_KEY).values[0]\n",
    "val_file_path = paths_df.get(VAL_KEY).values[0]\n",
    "\n",
    "original_train_df = pd.read_json(train_file_path, lines=True)\n",
    "original_val_df = pd.read_json(val_file_path, lines=True)\n",
    "original_test_df = pd.read_json(test_file_path, lines=True)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, train_file_path, len(original_train_df)],\n",
    "        [TEST_KEY, test_file_path, len(original_val_df)],\n",
    "        [VAL_KEY, val_file_path, len(original_test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Path', 'Size']\n",
    ")\n",
    "print('Original datasets:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "def enrich_df(df) -> None:\n",
    "    df['sentence'] = df[SENTENCE_COLS].apply(lambda row: ''.join(row).lower(), axis=1)\n",
    "    df['l'] = df[['label']].apply(lambda label: 1 if label[0] else 0, axis=1)\n",
    "\n",
    "random_index = original_train_df.sample(frac=TRAIN_FRAC, random_state=RANDOM_SEED).index\n",
    "test_df0 = original_train_df[~original_train_df.index.isin(random_index)]\n",
    "train_df = original_train_df[original_train_df.index.isin(random_index)].reset_index(drop=True)\n",
    "\n",
    "random_index = original_val_df.sample(frac=VAL_FRAC, random_state=RANDOM_SEED).index\n",
    "test_df1 = original_val_df[~original_val_df.index.isin(random_index)]\n",
    "val_df = original_val_df[original_val_df.index.isin(random_index)].reset_index(drop=True)\n",
    "\n",
    "test_df = pd.concat([test_df0, test_df1], ignore_index=True)\n",
    "\n",
    "enrich_df(train_df)\n",
    "enrich_df(val_df)\n",
    "enrich_df(test_df)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, len(train_df)],\n",
    "        [TEST_KEY, len(val_df)],\n",
    "        [VAL_KEY, len(test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Size']\n",
    ")\n",
    "print('Used datasets:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "print('Loading & preparation of dataframes are done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70889b",
   "metadata": {},
   "source": [
    "## Define max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dabc41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of max_length:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Raw</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Binary-based</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>By-threshold</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Used</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sentences is defined.\n"
     ]
    }
   ],
   "source": [
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_df.sentence.values, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(val_df.sentence.values, raw_max_length)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_df.sentence.values, raw_max_length)\n",
    "\n",
    "binary_based_max_length = define_max_length(raw_max_length, 1)\n",
    "if USE_BY_THRESHOLD:\n",
    "    max_length = TOKEN_SIZE_THRESHOLD\n",
    "else:\n",
    "    max_length = raw_max_length if USE_RAW_MAX_LENGTH else binary_based_max_length\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        ['Raw', raw_max_length],\n",
    "        ['Binary-based', binary_based_max_length],\n",
    "        ['By-threshold', TOKEN_SIZE_THRESHOLD],\n",
    "        ['Used', max_length]\n",
    "    ],\n",
    "    columns=['Type', 'Size']\n",
    ")\n",
    "print('Definition of max_length:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "print('Maximum length of sentences is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de36a4",
   "metadata": {},
   "source": [
    "## Datasets & dataloaders creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb63c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets & dataloaders creation is done\n"
     ]
    }
   ],
   "source": [
    "class T5TestDataset(TorchDataset):\n",
    "    def __init__(self, text, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._text.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item])\n",
    "        return {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=self._length)\n",
    "\n",
    "\n",
    "class T5TrainDataset(TorchDataset):\n",
    "    POS_LABEL = 'верно'\n",
    "    NEG_LABEL = 'неверно'\n",
    "\n",
    "    def __init__(self, text, label, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._label = label.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._label.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item], self._length)\n",
    "        output = {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "        label = self.POS_LABEL if self._label[item] == 1 else self.NEG_LABEL\n",
    "        label = self._tokenize(label, length=2).input_ids.reshape(-1).to(self._device)\n",
    "\n",
    "        output.update({'labels': label})\n",
    "        return output\n",
    "\n",
    "    def _tokenize(self, text, length):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=length)\n",
    "\n",
    "    \n",
    "train_dataset = T5TrainDataset(train_df['sentence'], train_df['l'], tokenizer, max_length, device)\n",
    "val_dataset = T5TrainDataset(val_df['sentence'], val_df['l'], tokenizer, max_length, device)\n",
    "test_dataset = T5TestDataset(test_df['sentence'], tokenizer, max_length, device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('Datasets & dataloaders creation is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb597a8",
   "metadata": {},
   "source": [
    "## Scheduler creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "193bfca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler is created\n"
     ]
    }
   ],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * EPOCHS\n",
    ")\n",
    "\n",
    "print('Scheduler is created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa2ee0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e00e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader len: 197\n",
      "\n",
      "======= Epoch 1 / 1 =======\n",
      "\n",
      "Training...\n",
      "\n",
      "0 2023-08-21 19:05:46.600186\n",
      "1 2023-08-21 19:06:20.228428\n",
      "2 2023-08-21 19:06:52.904405\n",
      "3 2023-08-21 19:07:26.064761\n",
      "4 2023-08-21 19:08:00.359646\n",
      "5 2023-08-21 19:08:32.811278\n",
      "6 2023-08-21 19:09:06.240639\n",
      "7 2023-08-21 19:09:38.657371\n",
      "8 2023-08-21 19:10:12.074097\n",
      "9 2023-08-21 19:10:44.506522\n",
      "\t\tBatch 10 of 197, loss : 7.695\n",
      "10 2023-08-21 19:11:17.909089\n",
      "11 2023-08-21 19:11:50.313613\n",
      "12 2023-08-21 19:12:23.717609\n",
      "13 2023-08-21 19:12:56.143276\n",
      "14 2023-08-21 19:13:29.539047\n",
      "15 2023-08-21 19:14:01.962787\n",
      "16 2023-08-21 19:14:35.340379\n",
      "17 2023-08-21 19:15:07.766043\n",
      "18 2023-08-21 19:15:41.178362\n",
      "19 2023-08-21 19:16:13.618604\n",
      "\t\tBatch 20 of 197, loss : 2.888\n",
      "20 2023-08-21 19:16:47.024691\n",
      "21 2023-08-21 19:17:19.440448\n",
      "22 2023-08-21 19:17:52.833131\n",
      "23 2023-08-21 19:18:25.258452\n",
      "24 2023-08-21 19:18:58.659251\n",
      "25 2023-08-21 19:19:31.072009\n",
      "26 2023-08-21 19:20:04.475222\n",
      "27 2023-08-21 19:20:36.896101\n",
      "28 2023-08-21 19:21:10.287093\n",
      "29 2023-08-21 19:21:42.690193\n",
      "\t\tBatch 30 of 197, loss : 2.737\n",
      "30 2023-08-21 19:22:16.088437\n",
      "31 2023-08-21 19:22:48.512364\n",
      "32 2023-08-21 19:23:21.908686\n",
      "33 2023-08-21 19:23:54.340969\n",
      "34 2023-08-21 19:24:27.711262\n",
      "35 2023-08-21 19:25:00.131614\n",
      "36 2023-08-21 19:25:33.541704\n",
      "37 2023-08-21 19:26:05.937729\n",
      "38 2023-08-21 19:26:39.348695\n",
      "39 2023-08-21 19:27:11.771084\n",
      "\t\tBatch 40 of 197, loss : 0.939\n",
      "40 2023-08-21 19:27:45.168728\n",
      "41 2023-08-21 19:28:17.579670\n",
      "42 2023-08-21 19:28:50.961742\n",
      "43 2023-08-21 19:29:23.388838\n",
      "44 2023-08-21 19:29:56.786630\n",
      "45 2023-08-21 19:30:29.214631\n",
      "46 2023-08-21 19:31:02.618763\n",
      "47 2023-08-21 19:31:35.035116\n",
      "48 2023-08-21 19:32:08.429699\n",
      "49 2023-08-21 19:32:40.844067\n",
      "\t\tBatch 50 of 197, loss : 0.695\n",
      "50 2023-08-21 19:33:14.236995\n",
      "51 2023-08-21 19:33:46.634105\n",
      "52 2023-08-21 19:34:20.040801\n",
      "53 2023-08-21 19:34:52.480624\n",
      "54 2023-08-21 19:35:25.873563\n",
      "55 2023-08-21 19:35:58.318097\n",
      "56 2023-08-21 19:36:31.716017\n",
      "57 2023-08-21 19:37:04.130914\n",
      "58 2023-08-21 19:37:37.519761\n",
      "59 2023-08-21 19:38:09.935280\n",
      "\t\tBatch 60 of 197, loss : 0.608\n",
      "60 2023-08-21 19:38:43.340286\n",
      "61 2023-08-21 19:39:15.751769\n",
      "62 2023-08-21 19:39:49.139287\n",
      "63 2023-08-21 19:40:21.540312\n",
      "64 2023-08-21 19:40:54.932957\n",
      "65 2023-08-21 19:41:27.364473\n",
      "66 2023-08-21 19:42:00.770141\n",
      "67 2023-08-21 19:42:33.200319\n",
      "68 2023-08-21 19:43:06.610024\n",
      "69 2023-08-21 19:43:39.048272\n",
      "\t\tBatch 70 of 197, loss : 0.393\n",
      "70 2023-08-21 19:44:12.451058\n",
      "71 2023-08-21 19:44:44.859619\n",
      "72 2023-08-21 19:45:18.284514\n",
      "73 2023-08-21 19:45:50.708201\n",
      "74 2023-08-21 19:46:24.109884\n",
      "75 2023-08-21 19:46:56.535905\n",
      "76 2023-08-21 19:47:29.933468\n",
      "77 2023-08-21 19:48:02.359872\n",
      "78 2023-08-21 19:48:35.753813\n",
      "79 2023-08-21 19:49:08.157633\n",
      "\t\tBatch 80 of 197, loss : 0.531\n",
      "80 2023-08-21 19:49:41.557674\n",
      "81 2023-08-21 19:50:13.971440\n",
      "82 2023-08-21 19:50:47.357106\n",
      "83 2023-08-21 19:51:19.761789\n",
      "84 2023-08-21 19:51:53.175100\n",
      "85 2023-08-21 19:52:25.581331\n",
      "86 2023-08-21 19:52:58.983258\n",
      "87 2023-08-21 19:53:31.376625\n",
      "88 2023-08-21 19:54:04.749779\n",
      "89 2023-08-21 19:54:37.196269\n",
      "\t\tBatch 90 of 197, loss : 0.779\n",
      "90 2023-08-21 19:55:10.605165\n",
      "91 2023-08-21 19:55:43.026208\n",
      "92 2023-08-21 19:56:16.420351\n",
      "93 2023-08-21 19:56:48.840992\n",
      "94 2023-08-21 19:57:22.246895\n",
      "95 2023-08-21 19:57:54.668336\n",
      "96 2023-08-21 19:58:28.071128\n",
      "97 2023-08-21 19:59:00.484077\n",
      "98 2023-08-21 19:59:33.870268\n",
      "99 2023-08-21 20:00:06.312776\n",
      "\t\tBatch 100 of 197, loss : 0.595\n",
      "100 2023-08-21 20:00:39.731473\n",
      "101 2023-08-21 20:01:12.187209\n",
      "102 2023-08-21 20:01:45.604738\n",
      "103 2023-08-21 20:02:18.072995\n",
      "104 2023-08-21 20:02:51.493599\n",
      "105 2023-08-21 20:03:23.945637\n",
      "106 2023-08-21 20:03:57.376521\n",
      "107 2023-08-21 20:04:29.806508\n",
      "108 2023-08-21 20:05:03.243064\n",
      "109 2023-08-21 20:05:35.670009\n",
      "\t\tBatch 110 of 197, loss : 0.441\n",
      "110 2023-08-21 20:06:09.077453\n",
      "111 2023-08-21 20:06:41.519260\n",
      "112 2023-08-21 20:07:14.935974\n",
      "113 2023-08-21 20:07:47.382918\n",
      "114 2023-08-21 20:08:20.791831\n",
      "115 2023-08-21 20:08:53.227907\n",
      "116 2023-08-21 20:09:26.619566\n",
      "117 2023-08-21 20:09:59.046553\n",
      "118 2023-08-21 20:10:32.439679\n",
      "119 2023-08-21 20:11:04.892242\n",
      "\t\tBatch 120 of 197, loss : 0.481\n",
      "120 2023-08-21 20:11:38.300917\n",
      "121 2023-08-21 20:12:10.758591\n",
      "122 2023-08-21 20:12:44.180732\n",
      "123 2023-08-21 20:13:16.611698\n",
      "124 2023-08-21 20:13:50.020894\n",
      "125 2023-08-21 20:14:22.461831\n",
      "126 2023-08-21 20:14:55.893158\n",
      "127 2023-08-21 20:15:28.316130\n",
      "128 2023-08-21 20:16:01.735473\n",
      "129 2023-08-21 20:16:34.178380\n",
      "\t\tBatch 130 of 197, loss : 0.300\n",
      "130 2023-08-21 20:17:07.585085\n",
      "131 2023-08-21 20:17:40.015916\n",
      "132 2023-08-21 20:18:13.428408\n",
      "133 2023-08-21 20:18:45.859644\n",
      "134 2023-08-21 20:19:19.258427\n",
      "135 2023-08-21 20:19:51.694083\n",
      "136 2023-08-21 20:20:25.125045\n",
      "137 2023-08-21 20:20:57.580872\n",
      "138 2023-08-21 20:21:31.001763\n",
      "139 2023-08-21 20:22:03.440388\n",
      "\t\tBatch 140 of 197, loss : 0.394\n",
      "140 2023-08-21 20:22:36.842977\n",
      "141 2023-08-21 20:23:09.298102\n",
      "142 2023-08-21 20:23:42.704840\n",
      "143 2023-08-21 20:24:15.146108\n",
      "144 2023-08-21 20:24:48.538132\n",
      "145 2023-08-21 20:25:20.967295\n",
      "146 2023-08-21 20:25:54.373661\n",
      "147 2023-08-21 20:26:26.818317\n",
      "148 2023-08-21 20:27:00.217419\n",
      "149 2023-08-21 20:27:32.649429\n",
      "\t\tBatch 150 of 197, loss : 0.380\n",
      "150 2023-08-21 20:28:06.057591\n",
      "151 2023-08-21 20:28:38.496608\n",
      "152 2023-08-21 20:29:11.905128\n",
      "153 2023-08-21 20:29:44.352590\n",
      "154 2023-08-21 20:30:17.754626\n",
      "155 2023-08-21 20:30:50.178290\n",
      "156 2023-08-21 20:31:23.584685\n",
      "157 2023-08-21 20:31:56.041894\n",
      "158 2023-08-21 20:32:29.477876\n",
      "159 2023-08-21 20:33:01.901645\n",
      "\t\tBatch 160 of 197, loss : 0.721\n",
      "160 2023-08-21 20:33:35.305935\n",
      "161 2023-08-21 20:34:07.733290\n",
      "162 2023-08-21 20:34:41.156238\n",
      "163 2023-08-21 20:35:13.604264\n",
      "164 2023-08-21 20:35:46.984541\n",
      "165 2023-08-21 20:36:19.432286\n",
      "166 2023-08-21 20:36:52.907643\n",
      "167 2023-08-21 20:37:25.336542\n",
      "168 2023-08-21 20:37:58.777535\n",
      "169 2023-08-21 20:38:31.226912\n",
      "\t\tBatch 170 of 197, loss : 0.622\n",
      "170 2023-08-21 20:39:04.655857\n",
      "171 2023-08-21 20:39:37.109955\n",
      "172 2023-08-21 20:40:10.537600\n",
      "173 2023-08-21 20:40:42.981383\n",
      "174 2023-08-21 20:41:16.401708\n",
      "175 2023-08-21 20:41:48.835229\n",
      "176 2023-08-21 20:42:22.455887\n",
      "177 2023-08-21 20:42:54.691972\n",
      "178 2023-08-21 20:43:28.113498\n",
      "179 2023-08-21 20:44:00.545385\n",
      "\t\tBatch 180 of 197, loss : 0.402\n",
      "180 2023-08-21 20:44:33.983639\n",
      "181 2023-08-21 20:45:06.434319\n",
      "182 2023-08-21 20:45:39.860858\n",
      "183 2023-08-21 20:46:12.303975\n",
      "184 2023-08-21 20:46:45.694489\n",
      "185 2023-08-21 20:47:18.113022\n",
      "186 2023-08-21 20:47:51.517337\n",
      "187 2023-08-21 20:48:23.931641\n",
      "188 2023-08-21 20:48:57.354017\n",
      "189 2023-08-21 20:49:29.817387\n",
      "\t\tBatch 190 of 197, loss : 0.357\n",
      "190 2023-08-21 20:50:03.228445\n",
      "191 2023-08-21 20:50:35.675048\n",
      "192 2023-08-21 20:51:09.094203\n",
      "193 2023-08-21 20:51:41.527971\n",
      "194 2023-08-21 20:52:14.955415\n",
      "195 2023-08-21 20:52:47.386572\n",
      "196 2023-08-21 20:53:08.511000\n",
      "\t\tBatch 197 of 197, loss : 0.390\n",
      "Validation...\n",
      "\tValidation loss: 0.3706040385589805\n",
      "Trainig complete!\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "# n_epochs = conf('train.epochs.t5')\n",
    "\n",
    "dl_length = len(train_dataloader)\n",
    "\n",
    "# <\n",
    "print(f'dataloader len: {dl_length}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n======= Epoch {epoch + 1} / {EPOCHS} =======\\n')\n",
    "\n",
    "    print('Training...\\n')\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # < del\n",
    "        print(f'{batch_id} {datetime.datetime.now()}')\n",
    "        if (((batch_id + 1) % 10 == 0) and not batch_id == 0) or (batch_id == dl_length - 1):\n",
    "            print(f'\\t\\tBatch {batch_id+1} of {dl_length}, loss : {loss.item():.3f}')\n",
    "        \n",
    "    print('Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loss = [model(**batch).loss.item() for batch in val_dataloader]\n",
    "        \n",
    "    print(f'\\tValidation loss: {np.sum(eval_loss)/len(eval_loss)}')\n",
    "    \n",
    "print('Trainig complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c679d5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a2bef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2023-08-22 21:55:46.496699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2023-08-22 21:55:47.620293\n",
      "2 2023-08-22 21:55:47.962932\n",
      "3 2023-08-22 21:55:48.303195\n",
      "4 2023-08-22 21:55:48.644253\n",
      "5 2023-08-22 21:55:48.985396\n",
      "6 2023-08-22 21:55:49.327234\n",
      "7 2023-08-22 21:55:49.671743\n",
      "8 2023-08-22 21:55:50.019898\n",
      "9 2023-08-22 21:55:50.362977\n",
      "10 2023-08-22 21:55:50.706875\n",
      "11 2023-08-22 21:55:51.052055\n",
      "12 2023-08-22 21:55:51.394132\n",
      "13 2023-08-22 21:55:51.735009\n",
      "14 2023-08-22 21:55:52.080592\n",
      "15 2023-08-22 21:55:52.429671\n",
      "16 2023-08-22 21:55:52.775660\n",
      "17 2023-08-22 21:55:53.119738\n",
      "18 2023-08-22 21:55:53.462799\n",
      "19 2023-08-22 21:55:53.805876\n",
      "20 2023-08-22 21:55:54.152954\n",
      "21 2023-08-22 21:55:54.500035\n",
      "22 2023-08-22 21:55:54.844032\n",
      "23 2023-08-22 21:55:55.188110\n",
      "24 2023-08-22 21:55:55.534187\n",
      "25 2023-08-22 21:55:55.879266\n",
      "26 2023-08-22 21:55:56.226312\n",
      "27 2023-08-22 21:55:56.571396\n",
      "28 2023-08-22 21:55:56.915487\n",
      "29 2023-08-22 21:55:57.261575\n",
      "30 2023-08-22 21:55:57.604571\n",
      "31 2023-08-22 21:55:57.951648\n",
      "32 2023-08-22 21:55:58.287828\n",
      "T5 testing is done, F1-score: 0.652\n"
     ]
    }
   ],
   "source": [
    "def test(model, tokenizer, dataloader):\n",
    "    pos_label = tokenizer(T5TrainDataset.POS_LABEL,\n",
    "                          return_tensors='pt',\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=2)['input_ids'][0][0].item()\n",
    "    model.eval()\n",
    "\n",
    "    result = np.array([])\n",
    "    # <\n",
    "    l = len(dataloader)\n",
    "    # <\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        # <\n",
    "        print(f'{batch_id} {datetime.datetime.now()}')\n",
    "        # <\n",
    "        tokens = model.generate(**batch)\n",
    "        tokens = [1 if pos_label in token else 0 for token in tokens]\n",
    "        result = np.hstack([result, tokens])\n",
    "    \n",
    "    return result\n",
    "\n",
    "if TESTING_FROM_FILES:\n",
    "    tokenizer_from_disk = T5Tokenizer.from_pretrained(SAVED_MODEL_PATH, use_fast=False)\n",
    "    model_from_disk = T5ForConditionalGeneration.from_pretrained(SAVED_MODEL_PATH)\n",
    "    model_from_disk.to(device)\n",
    "    result = test(model_from_disk, tokenizer_from_disk, test_dataloader)\n",
    "else:\n",
    "    result = test(model, tokenizer, test_dataloader)\n",
    "\n",
    "f1 = f1_score(result, test_df['l'])\n",
    "\n",
    "print(f'T5 testing is done, F1-score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71684227",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c20881ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving is done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.isdir(SAVED_MODEL_PATH):\n",
    "    for file_name in os.listdir(SAVED_MODEL_PATH):\n",
    "        file_path = os.path.join(SAVED_MODEL_PATH, file_name)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "tokenizer.save_pretrained(SAVED_MODEL_PATH)\n",
    "tokenizer.save_vocabulary(SAVED_MODEL_PATH)\n",
    "model.save_pretrained(SAVED_MODEL_PATH)\n",
    "\n",
    "print('Saving is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c194bf",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
