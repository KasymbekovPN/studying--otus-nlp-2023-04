{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b952b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc22f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification\n",
    ")\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf3189",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a2f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants are initialized.\n"
     ]
    }
   ],
   "source": [
    "# INPUT_PATHS = 'paths.json'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TRAIN_KEY = 'train.jsonl'\n",
    "TEST_KEY = 'test.jsonl'\n",
    "VAL_KEY = 'val.jsonl'\n",
    "\n",
    "DATA_DIRECTORY = '.\\\\data'\n",
    "# RUSSE_INPUT = os.path.join(DATA_DIRECTORY, 'russe_paths.json') #<\n",
    "DANETQA_INPUT = os.path.join(DATA_DIRECTORY, 'danetqa_paths.json')\n",
    "\n",
    "PREPARED_BERT_PATH = 'ai-forever/ruBert-base'\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "\n",
    "print('Constants are initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bab601",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED) # check needness\n",
    "np.random.seed(RANDOM_SEED) # check needness\n",
    "torch.manual_seed(RANDOM_SEED) # check needness\n",
    "torch.cuda.manual_seed(RANDOM_SEED) # check needness\n",
    "\n",
    "print('Random seed is set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131a912",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439aad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Device is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74fa9",
   "metadata": {},
   "source": [
    "## Model & tokenizer & optimizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7614d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, optimizer are created.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PREPARED_BERT_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PREPARED_BERT_PATH,\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "print('Model, tokenizer, optimizer are created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bc0d0",
   "metadata": {},
   "source": [
    "## Loading original dataframes from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f71d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Path</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train.jsonl</td>\n",
       "      <td>.\\data\\DaNetQA\\train.jsonl</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test.jsonl</td>\n",
       "      <td>.\\data\\DaNetQA\\test.jsonl</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val.jsonl</td>\n",
       "      <td>.\\data\\DaNetQA\\val.jsonl</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_df = pd.read_json(DANETQA_INPUT, lines=True)\n",
    "# paths_df = pd.read_json('./data/danetqa_paths.json', lines=True)\n",
    "\n",
    "train_file_path = paths_df.get(TRAIN_KEY).values[0]\n",
    "test_file_path = paths_df.get(TEST_KEY).values[0]\n",
    "val_file_path = paths_df.get(VAL_KEY).values[0]\n",
    "\n",
    "original_train_df = pd.read_json(train_file_path, lines=True)\n",
    "original_val_df = pd.read_json(val_file_path, lines=True)\n",
    "original_test_df = pd.read_json(test_file_path, lines=True)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, train_file_path, len(original_train_df)],\n",
    "        [TEST_KEY, test_file_path, len(original_val_df)],\n",
    "        [VAL_KEY, val_file_path, len(original_test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Path', 'size']\n",
    ")\n",
    "print('Datasets:')\n",
    "HTML(output_df.to_html(index=False))\n",
    "\n",
    "# original_train_df = pd.read_json(train_file_path, lines=True)\n",
    "# original_val_df = pd.read_json(val_file_path, lines=True)\n",
    "# original_test_df = pd.read_json(val_test_path, lines=True)\n",
    "\n",
    "# output_df = pd.DataFrame(\n",
    "#     [[TRAIN_KEY, len(original_train_df)], [TEST_KEY, test_file_path], [VAL_KEY, val_file_path]],\n",
    "#     columns=['Key', 'Path']\n",
    "# )\n",
    "# print('Received next paths:')\n",
    "# HTML(output_df.to_html(index=False))\n",
    "\n",
    "\n",
    "# print(type(original_train_df))\n",
    "# print(len(original_train_df))\n",
    "# print(original_train_df.shape)\n",
    "\n",
    "# df = pd.DataFrame(data, columns=['Type', 'F1'])\n",
    "# HTML(df.to_html(index=False))\n",
    "\n",
    "# train_eval_dataframe = pd.read_csv(conf('dataset.path.train'), usecols=conf('dataset.usecols'))\n",
    "# test_dataframe = pd.read_csv(conf('dataset.path.test'), usecols=conf('dataset.usecols'))\n",
    "\n",
    "# random_index = train_eval_dataframe.sample(frac=conf('train.size'), random_state=conf('random.seed')).index\n",
    "# eval_dataframe = train_eval_dataframe[~train_eval_dataframe.index.isin(random_index)]\n",
    "# train_dataframe = train_eval_dataframe[train_eval_dataframe.index.isin(random_index)]\n",
    "\n",
    "# print('Dataframes are loaded.')\n",
    "# print(f'Train dataframe size: {len(train_dataframe)}')\n",
    "# print(f'Evaluate dataframe size: {len(eval_dataframe)}')\n",
    "# print(f'Test dataframe size: {len(test_dataframe)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35603e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
