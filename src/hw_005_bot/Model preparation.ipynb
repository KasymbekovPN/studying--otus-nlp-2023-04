{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b952b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc22f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf3189",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8a2f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants are initialized.\n"
     ]
    }
   ],
   "source": [
    "# INPUT_PATHS = 'paths.json'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TRAIN_KEY = 'train.jsonl'\n",
    "TEST_KEY = 'test.jsonl'\n",
    "VAL_KEY = 'val.jsonl'\n",
    "\n",
    "DATA_DIRECTORY = '.\\\\data'\n",
    "# RUSSE_INPUT = os.path.join(DATA_DIRECTORY, 'russe_paths.json') #<\n",
    "DANETQA_INPUT = os.path.join(DATA_DIRECTORY, 'danetqa_paths.json')\n",
    "\n",
    "print('Constants are initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bab601",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED) # check needness\n",
    "np.random.seed(RANDOM_SEED) # check needness\n",
    "torch.manual_seed(RANDOM_SEED) # check needness\n",
    "torch.cuda.manual_seed(RANDOM_SEED) # check needness\n",
    "\n",
    "print('Random seed is set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131a912",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "439aad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Device is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74fa9",
   "metadata": {},
   "source": [
    "## Model & tokenizer & optimizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7614d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(conf('pretrained.path.bert'))\n",
    "# bert_model = BertForSequenceClassification.from_pretrained(\n",
    "#     conf('pretrained.path.bert'),\n",
    "#     num_labels=2,\n",
    "#     output_attentions=False,\n",
    "#     output_hidden_states=False\n",
    "# )\n",
    "# bert_optimizer = AdamW(bert_model.parameters(), lr=conf('optimizer.lr.bert'), eps=conf('optimizer.eps.bert'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bc0d0",
   "metadata": {},
   "source": [
    "## Loading dataframes from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f71d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received next paths:\n",
      "\ttrain.jsonl : .\\data\\DaNetQA\\train.jsonl\n",
      "\ttest.jsonl : .\\data\\DaNetQA\\test.jsonl\n",
      "\tval.jsonl : .\\data\\DaNetQA\\val.jsonl\n"
     ]
    }
   ],
   "source": [
    "paths_df = pd.read_json(DANETQA_INPUT, lines=True)\n",
    "# paths_df = pd.read_json('./data/danetqa_paths.json', lines=True)\n",
    "\n",
    "train_file_path = paths_df.get(TRAIN_KEY).values[0]\n",
    "test_file_path = paths_df.get(TEST_KEY).values[0]\n",
    "val_file_path = paths_df.get(VAL_KEY).values[0]\n",
    "\n",
    "print('Received next paths:')\n",
    "print(f'\\t{TRAIN_KEY} : {train_file_path}')\n",
    "print(f'\\t{TEST_KEY} : {test_file_path}')\n",
    "print(f'\\t{VAL_KEY} : {val_file_path}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_eval_dataframe = pd.read_csv(conf('dataset.path.train'), usecols=conf('dataset.usecols'))\n",
    "# test_dataframe = pd.read_csv(conf('dataset.path.test'), usecols=conf('dataset.usecols'))\n",
    "\n",
    "# random_index = train_eval_dataframe.sample(frac=conf('train.size'), random_state=conf('random.seed')).index\n",
    "# eval_dataframe = train_eval_dataframe[~train_eval_dataframe.index.isin(random_index)]\n",
    "# train_dataframe = train_eval_dataframe[train_eval_dataframe.index.isin(random_index)]\n",
    "\n",
    "# print('Dataframes are loaded.')\n",
    "# print(f'Train dataframe size: {len(train_dataframe)}')\n",
    "# print(f'Evaluate dataframe size: {len(eval_dataframe)}')\n",
    "# print(f'Test dataframe size: {len(test_dataframe)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35603e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
