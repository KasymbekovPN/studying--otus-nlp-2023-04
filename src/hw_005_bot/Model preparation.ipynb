{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b952b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc22f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf3189",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a2f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants are initialized.\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "TRAIN_KEY = 'train'\n",
    "TEST_KEY = 'test'\n",
    "VAL_KEY = 'val'\n",
    "\n",
    "DATA_DIRECTORY = '.\\\\data'\n",
    "DANETQA_INPUT = os.path.join(DATA_DIRECTORY, 'danetqa_paths.json')\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join(DATA_DIRECTORY, 'saved')\n",
    "\n",
    "PREPARED_BERT_PATH = 'ai-forever/ruBert-base'\n",
    "PREPARED_T5_BASE_PATH = 'ai-forever/ruT5-base'\n",
    "\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "TRAIN_FRAC = 0.9\n",
    "VAL_FRAC = 0.9\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "\n",
    "SENTENCE_COLS = ['question', 'passage']\n",
    "\n",
    "USE_RAW_MAX_LENGTH = True\n",
    "USE_BY_THRESHOLD = True\n",
    "TOKEN_SIZE_THRESHOLD = 300\n",
    "\n",
    "TESTING_FROM_FILES = True\n",
    "\n",
    "print('Constants are initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bab601",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED) # check needness\n",
    "np.random.seed(RANDOM_SEED) # check needness\n",
    "torch.manual_seed(RANDOM_SEED) # check needness\n",
    "torch.cuda.manual_seed(RANDOM_SEED) # check needness\n",
    "\n",
    "print('Random seed is set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131a912",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439aad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Device is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74fa9",
   "metadata": {},
   "source": [
    "## Model & tokenizer & optimizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7614d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, optimizer are created.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(PREPARED_T5_BASE_PATH, use_fast=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(PREPARED_T5_BASE_PATH)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "\n",
    "print('Model, tokenizer, optimizer are created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bc0d0",
   "metadata": {},
   "source": [
    "## Loading & preparation of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f71d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Path</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>.\\data\\DaNetQA\\train.jsonl</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>.\\data\\DaNetQA\\test.jsonl</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val</td>\n",
       "      <td>.\\data\\DaNetQA\\val.jsonl</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Key</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>val</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & preparation of dataframes are done.\n"
     ]
    }
   ],
   "source": [
    "paths_df = pd.read_json(DANETQA_INPUT, lines=True)\n",
    "\n",
    "train_file_path = paths_df.get(TRAIN_KEY).values[0]\n",
    "test_file_path = paths_df.get(TEST_KEY).values[0]\n",
    "val_file_path = paths_df.get(VAL_KEY).values[0]\n",
    "\n",
    "original_train_df = pd.read_json(train_file_path, lines=True)\n",
    "original_val_df = pd.read_json(val_file_path, lines=True)\n",
    "original_test_df = pd.read_json(test_file_path, lines=True)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, train_file_path, len(original_train_df)],\n",
    "        [TEST_KEY, test_file_path, len(original_val_df)],\n",
    "        [VAL_KEY, val_file_path, len(original_test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Path', 'Size']\n",
    ")\n",
    "print('Original datasets:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "def enrich_df(df) -> None:\n",
    "    df['sentence'] = df[SENTENCE_COLS].apply(lambda row: ''.join(row).lower(), axis=1)\n",
    "    df['l'] = df[['label']].apply(lambda label: 1 if label[0] else 0, axis=1)\n",
    "\n",
    "random_index = original_train_df.sample(frac=TRAIN_FRAC, random_state=RANDOM_SEED).index\n",
    "test_df0 = original_train_df[~original_train_df.index.isin(random_index)]\n",
    "train_df = original_train_df[original_train_df.index.isin(random_index)].reset_index(drop=True)\n",
    "\n",
    "random_index = original_val_df.sample(frac=VAL_FRAC, random_state=RANDOM_SEED).index\n",
    "test_df1 = original_val_df[~original_val_df.index.isin(random_index)]\n",
    "val_df = original_val_df[original_val_df.index.isin(random_index)].reset_index(drop=True)\n",
    "\n",
    "test_df = pd.concat([test_df0, test_df1], ignore_index=True)\n",
    "\n",
    "enrich_df(train_df)\n",
    "enrich_df(val_df)\n",
    "enrich_df(test_df)\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        [TRAIN_KEY, len(train_df)],\n",
    "        [TEST_KEY, len(val_df)],\n",
    "        [VAL_KEY, len(test_df)]\n",
    "    ],\n",
    "    columns=['Key', 'Size']\n",
    ")\n",
    "print('Used datasets:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "print('Loading & preparation of dataframes are done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70889b",
   "metadata": {},
   "source": [
    "## Define max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabc41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of max_length:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Raw</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Binary-based</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>By-threshold</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Used</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sentences is defined.\n"
     ]
    }
   ],
   "source": [
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_df.sentence.values, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(val_df.sentence.values, raw_max_length)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_df.sentence.values, raw_max_length)\n",
    "\n",
    "binary_based_max_length = define_max_length(raw_max_length, 1)\n",
    "if USE_BY_THRESHOLD:\n",
    "    max_length = TOKEN_SIZE_THRESHOLD\n",
    "else:\n",
    "    max_length = raw_max_length if USE_RAW_MAX_LENGTH else binary_based_max_length\n",
    "\n",
    "output_df = pd.DataFrame(\n",
    "    [\n",
    "        ['Raw', raw_max_length],\n",
    "        ['Binary-based', binary_based_max_length],\n",
    "        ['By-threshold', TOKEN_SIZE_THRESHOLD],\n",
    "        ['Used', max_length]\n",
    "    ],\n",
    "    columns=['Type', 'Size']\n",
    ")\n",
    "print('Definition of max_length:')\n",
    "display(HTML(output_df.to_html(index=False)))\n",
    "\n",
    "print('Maximum length of sentences is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de36a4",
   "metadata": {},
   "source": [
    "## Datasets & dataloaders creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb63c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets & dataloaders creation is done\n"
     ]
    }
   ],
   "source": [
    "class T5TestDataset(TorchDataset):\n",
    "    def __init__(self, text, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        #<\n",
    "#         print('len')\n",
    "        return self._text.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        #< \n",
    "#         print('__getitem__')\n",
    "        output = self._tokenize(self._text[item])\n",
    "        return {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        #< \n",
    "#         print('_tokenize')\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=self._length)\n",
    "\n",
    "\n",
    "class T5TrainDataset(TorchDataset):\n",
    "    POS_LABEL = 'верно'\n",
    "    NEG_LABEL = 'неверно'\n",
    "\n",
    "    def __init__(self, text, label, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._label = label.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._label.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item], self._length)\n",
    "        output = {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "        label = self.POS_LABEL if self._label[item] == 1 else self.NEG_LABEL\n",
    "        label = self._tokenize(label, length=2).input_ids.reshape(-1).to(self._device)\n",
    "\n",
    "        output.update({'labels': label})\n",
    "        return output\n",
    "\n",
    "    def _tokenize(self, text, length):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=length)\n",
    "\n",
    "    \n",
    "train_dataset = T5TrainDataset(train_df['sentence'], train_df['l'], tokenizer, max_length, device)\n",
    "val_dataset = T5TrainDataset(val_df['sentence'], val_df['l'], tokenizer, max_length, device)\n",
    "test_dataset = T5TestDataset(test_df['sentence'], tokenizer, max_length, device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('Datasets & dataloaders creation is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb597a8",
   "metadata": {},
   "source": [
    "## Scheduler creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193bfca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler is created\n"
     ]
    }
   ],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * EPOCHS\n",
    ")\n",
    "\n",
    "print('Scheduler is created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa2ee0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e00e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 1 =======\n",
      "\n",
      "Training...\n",
      "\n",
      "\t\tBatch 10 of 50, loss : 5.585\n",
      "\t\tBatch 20 of 50, loss : 2.458\n",
      "\t\tBatch 30 of 50, loss : 1.766\n",
      "\t\tBatch 40 of 50, loss : 0.740\n",
      "\t\tBatch 50 of 50, loss : 1.253\n",
      "Validation...\n",
      "\tValidation loss: 0.4562477357685566\n",
      "Trainig complete!\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "# n_epochs = conf('train.epochs.t5')\n",
    "\n",
    "dl_length = len(train_dataloader)\n",
    "\n",
    "# <\n",
    "# print(f'dataloader len: {dl_length}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n======= Epoch {epoch + 1} / {EPOCHS} =======\\n')\n",
    "\n",
    "    print('Training...\\n')\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # < del\n",
    "#         print(f'{batch_id} {datetime.datetime.now()}')\n",
    "        if (((batch_id + 1) % 10 == 0) and not batch_id == 0) or (batch_id == dl_length - 1):\n",
    "            print(f'\\t\\tBatch {batch_id+1} of {dl_length}, loss : {loss.item():.3f}')\n",
    "        \n",
    "    print('Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loss = [model(**batch).loss.item() for batch in val_dataloader]\n",
    "        \n",
    "    print(f'\\tValidation loss: {np.sum(eval_loss)/len(eval_loss)}')\n",
    "    \n",
    "print('Trainig complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c679d5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2bef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len\n",
      "len\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "0 2023-08-27 16:35:39.268082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "1 2023-08-27 16:35:39.645482\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "2 2023-08-27 16:35:40.020045\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "3 2023-08-27 16:35:40.399173\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "4 2023-08-27 16:35:40.776805\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "5 2023-08-27 16:35:41.158512\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "6 2023-08-27 16:35:41.541153\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "__getitem__\n",
      "_tokenize\n",
      "7 2023-08-27 16:35:41.916128\n",
      "__getitem__\n",
      "_tokenize\n",
      "8 2023-08-27 16:35:42.276789\n",
      "T5 testing is done, F1-score: 0.758\n"
     ]
    }
   ],
   "source": [
    "def test(model, tokenizer, dataloader):\n",
    "    pos_label = tokenizer(T5TrainDataset.POS_LABEL,\n",
    "                          return_tensors='pt',\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=2)['input_ids'][0][0].item()\n",
    "    model.eval()\n",
    "\n",
    "    result = np.array([])\n",
    "    # <\n",
    "    l = len(dataloader)\n",
    "    # <\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        # <\n",
    "#         print(batch)\n",
    "        print(f'{batch_id} {datetime.datetime.now()}')\n",
    "        # <\n",
    "        tokens = model.generate(**batch)\n",
    "#         print(tokens)\n",
    "        tokens = [1 if pos_label in token else 0 for token in tokens]\n",
    "#         print(tokens)\n",
    "        result = np.hstack([result, tokens])\n",
    "    \n",
    "    return result\n",
    "\n",
    "if TESTING_FROM_FILES:\n",
    "    tokenizer_from_disk = T5Tokenizer.from_pretrained(SAVED_MODEL_PATH, use_fast=False)\n",
    "    model_from_disk = T5ForConditionalGeneration.from_pretrained(SAVED_MODEL_PATH)\n",
    "    model_from_disk.to(device)\n",
    "    result = test(model_from_disk, tokenizer_from_disk, test_dataloader)\n",
    "else:\n",
    "    result = test(model, tokenizer, test_dataloader)\n",
    "\n",
    "f1 = f1_score(result, test_df['l'])\n",
    "\n",
    "print(f'T5 testing is done, F1-score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71684227",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20881ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving is done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.isdir(SAVED_MODEL_PATH):\n",
    "    for file_name in os.listdir(SAVED_MODEL_PATH):\n",
    "        file_path = os.path.join(SAVED_MODEL_PATH, file_name)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "tokenizer.save_pretrained(SAVED_MODEL_PATH)\n",
    "tokenizer.save_vocabulary(SAVED_MODEL_PATH)\n",
    "model.save_pretrained(SAVED_MODEL_PATH)\n",
    "\n",
    "print('Saving is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c194bf",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
