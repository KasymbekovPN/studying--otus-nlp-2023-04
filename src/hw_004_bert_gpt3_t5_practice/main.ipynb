{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5429398",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc0b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import random\n",
    "import wget\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from shutil import rmtree\n",
    "from datasets import load_metric, Dataset, DatasetDict\n",
    "from razdel import tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, BertTokenizer, AutoModelForMaskedLM, BertForSequenceClassification, \\\n",
    "                         get_linear_schedule_with_warmup, AutoModel, AutoModelForCausalLM, DataCollatorForSeq2Seq, \\\n",
    "                         Seq2SeqTrainingArguments, Seq2SeqTrainer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d11c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a344298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration block is done.\n"
     ]
    }
   ],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'random.seed': 42,\n",
    "            \n",
    "            'url.dataset.train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',\n",
    "            'url.dataset.test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true',\n",
    "            \n",
    "            'path.dataset.train': './train_dataset.csv',\n",
    "            'path.dataset.test': './test_dataset.csv',\n",
    "            \n",
    "            'name.train': 'TRAIN',\n",
    "            'name.test': 'TEST',\n",
    "  \n",
    "            'dataframe.train.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "            'dataframe.test.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "            \n",
    "            'bert.train-size': 0.9,\n",
    "            'bert.batch-size': 32,\n",
    "            'bert.optimizer.learning-rate': 2e-5,\n",
    "            'bert.optimizer.eps': 1e-8,\n",
    "#             'bert.train.epochs': 2\n",
    "            'bert.train.epochs': 1,\n",
    "            'bert.pretrain.path': 'ai-forever/ruBert-base',\n",
    "#             'bert.pretrain.path': 'DeepPavlov/rubert-base-cased',\n",
    "            \n",
    "            'gpt.pretrain.path': 'ai-forever/rugpt3large_based_on_gpt2',\n",
    "            \n",
    "            't5.model-name': 't5-base',\n",
    "            't5.pretrain.path': 'ai-forever/ruT5-base',\n",
    "            't5.n-seeds': 1,\n",
    "            't5.n-epochs': 20,\n",
    "            't5.lr-values': (1e-4, 1e-3),\n",
    "            't5.decay-vales': (0, 1e-4),\n",
    "            't5.batch-sizes': (128,),\n",
    "            't5.label.pos': 'yes',\n",
    "            't5.label.neg': 'no',\n",
    "            't5.dataset.url.train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',\n",
    "            't5.dataset.url.in-domain-dev': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true',\n",
    "            't5.dataset.url.out-of-domain-dev': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/out_of_domain_dev.csv?raw=true',\n",
    "            't5.dataset.url.test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/test.csv?raw=true',\n",
    "            't5.dataset.path.train': './data/in_domain_train.csv',\n",
    "            't5.dataset.path.in-domain-dev': './data/in_domain_dev.csv',\n",
    "            't5.dataset.path.out-of-domain-dev': './data/out_of_domain_dev.csv',\n",
    "            't5.dataset.path.test': './data/test.csv',\n",
    "            't5.const-training-args': {\n",
    "                'overwrite_output_dir': True,\n",
    "                'evaluation_strategy': 'epoch',\n",
    "                'lr_scheduler_type': 'constant',\n",
    "                'save_strategy': 'epoch',\n",
    "                'save_total_limit': 1,\n",
    "                'fp16': True,\n",
    "                'dataloader_num_workers': 4,\n",
    "                'group_by_length': True,\n",
    "                'report_to': 'none',\n",
    "                'load_best_model_at_end': True,\n",
    "                'metric_for_best_model': 'eval_mcc',\n",
    "                'optim': 'adafactor',\n",
    "                'predict_with_generate': True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args) == 0 or args[0] not in self._params:\n",
    "            return None\n",
    "        return self._params[args[0]]\n",
    "    \n",
    "    def check(self, *args):\n",
    "        result = True\n",
    "        absence_params = set()\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str) and arg not in self._params:\n",
    "                result = False\n",
    "                absence_params.add(arg)\n",
    "        message = 'Absence params: ' + ', '.join(absence_params)\n",
    "        assert result, message\n",
    "        \n",
    "        \n",
    "conf = Configurator()\n",
    "\n",
    "print('Configuration block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488cf80",
   "metadata": {},
   "source": [
    "## Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97e7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions block is done.\n"
     ]
    }
   ],
   "source": [
    "class CList(Enum):\n",
    "    SET_RANDOM_SEED = 0\n",
    "    DEVICE_DEFINED = 1\n",
    "    DATASET_DOWNLOADED = 2\n",
    "    DATASET_LOADED = 3\n",
    "    DATASET_PREPARED = 4\n",
    "    TOKENIZERS_MODELS_CREATED = 5\n",
    "    BERT_SCHEDULER_CREATED = 6\n",
    "    BERT_TRAINED = 7\n",
    "    BERT_TESTED = 8\n",
    "    GPT_PREPARED = 9\n",
    "\n",
    "class Conditions:    \n",
    "    def __init__(self) -> None:\n",
    "        self._conditions = {}\n",
    "        \n",
    "    def set(self, message, *conditions):\n",
    "        for condition in conditions:\n",
    "            self._conditions[condition] = True\n",
    "        print(message)\n",
    "    \n",
    "    def check(self, *conditions):\n",
    "        result = True\n",
    "        absence = set()\n",
    "        for condition in conditions:\n",
    "            if condition not in self._conditions:\n",
    "                result = False\n",
    "                absence.add(condition.name)\n",
    "        fail_message = 'Absence conditions: ' + ', '.join(absence)\n",
    "        assert result, fail_message\n",
    "\n",
    "conds = Conditions()\n",
    "\n",
    "print('Conditions block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b1e38",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6121820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "conf.check('random.seed')\n",
    "\n",
    "random.seed(conf('random.seed'))\n",
    "np.random.seed(conf('random.seed'))\n",
    "torch.manual_seed(conf('random.seed'))\n",
    "torch.cuda.manual_seed(conf('random.seed'))\n",
    "\n",
    "conds.set('Random seed is set.', CList.SET_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aeeff9",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f137c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.SET_RANDOM_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "conds.set(\"Device is defined\", CList.DEVICE_DEFINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572c92a",
   "metadata": {},
   "source": [
    "## Models & tokenizers & optimizers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a287bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models & tokenizers creation is done.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.optimizer.learning-rate', 'bert.optimizer.eps', 'bert.pretrain.path', 'gpt.pretrain.path')\n",
    "conds.check(CList.DEVICE_DEFINED)\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(conf('bert.pretrain.path'))\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    conf('bert.pretrain.path'),\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=conf('bert.optimizer.learning-rate'), eps=conf('bert.optimizer.eps'))\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(conf('gpt.pretrain.path'))\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(conf('gpt.pretrain.path'))\n",
    "\n",
    "conds.set('Models & tokenizers creation is done.', CList.TOKENIZERS_MODELS_CREATED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b4c8e",
   "metadata": {},
   "source": [
    "## Downloading datasets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88e4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"TRAIN\" is already downloaded.\n",
      "Dataset \"TEST\" is already downloaded.\n",
      "Datasets downloading is done\n"
     ]
    }
   ],
   "source": [
    "# optimize checking  !!!\n",
    "conf.check(\n",
    "    'url.dataset.train',\n",
    "    'path.dataset.train',\n",
    "    'name.train',\n",
    "    'url.dataset.test',\n",
    "    'path.dataset.test',\n",
    "    'name.test'\n",
    ");\n",
    "conds.check(CList.TOKENIZERS_MODELS_CREATED)\n",
    "\n",
    "def load_dataset(url: str, path: str, name: str):\n",
    "    if os.path.exists(path):\n",
    "        print('Dataset \"' + name + '\" is already downloaded.')\n",
    "    else:\n",
    "        wget.download(url, path)\n",
    "        print(' Dataset \"' + name + '\" is downloaded.')\n",
    "        \n",
    "load_dataset(conf('url.dataset.train'), conf('path.dataset.train'), conf('name.train'))\n",
    "load_dataset(conf('url.dataset.test'), conf('path.dataset.test'), conf('name.test'))\n",
    "conds.set('Datasets downloading is done', CList.DATASET_DOWNLOADED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31748d",
   "metadata": {},
   "source": [
    "## Loading datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e4c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "conf.check(\n",
    "    'path.dataset.train',\n",
    "    'path.dataset.test',\n",
    "    'dataframe.train.names',\n",
    "    'dataframe.test.names'\n",
    ")\n",
    "conds.check(CList.DATASET_DOWNLOADED)\n",
    "\n",
    "train_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.train'),\n",
    "    names=conf('dataframe.train.names'),\n",
    "    skiprows=1,\n",
    "    usecols=conf('dataframe.train.usecols')\n",
    ")\n",
    "\n",
    "test_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.test'),\n",
    "    names=conf('dataframe.test.names'),\n",
    "    skiprows=1,\n",
    "    usecols=conf('dataframe.test.usecols')\n",
    ")\n",
    "conds.set('Datasets are loaded from disk.', CList.DATASET_LOADED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d81895",
   "metadata": {},
   "source": [
    "## BERT datasets preparation, create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05af8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT datasets are prepared.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.batch-size', 'bert.batch-size', 'bert.batch-size')\n",
    "conds.check(CList.DATASET_LOADED)\n",
    "\n",
    "train_sentences = train_dataframe.sentence.values\n",
    "train_acceptables = train_dataframe.acceptable.values\n",
    "test_sentences = test_dataframe.sentence.values\n",
    "test_acceptables = test_dataframe.acceptable.values\n",
    "\n",
    "\n",
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = bert_tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_sentences, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_sentences, raw_max_length)\n",
    "\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "\n",
    "max_length = define_max_length(raw_max_length, 1)\n",
    "\n",
    "def create_bert_dataset(sentences, acceptables, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = bert_tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    acceptables = torch.tensor(acceptables)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, acceptables)\n",
    "\n",
    "train_val_dataset = create_bert_dataset(train_sentences, train_acceptables, max_length)\n",
    "test_dataset = create_bert_dataset(test_sentences, test_acceptables, max_length);\n",
    "\n",
    "train_val_dataset_size = len(train_val_dataset)\n",
    "train_size = int(conf('bert.train-size') * train_val_dataset_size)\n",
    "val_size = train_val_dataset_size - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=conf('bert.batch-size'))\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=conf('bert.batch-size'))\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=conf('bert.batch-size'))\n",
    "\n",
    "conds.set('BERT datasets are prepared.', CList.DATASET_PREPARED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ae05e",
   "metadata": {},
   "source": [
    "## BERT scheduler creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c4a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT scheduler is created.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.train.epochs')\n",
    "conds.check(CList.DATASET_PREPARED)\n",
    "\n",
    "bert_scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * conf('bert.train.epochs')\n",
    ")\n",
    "\n",
    "conds.set('BERT scheduler is created.', CList.BERT_SCHEDULER_CREATED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe30d1",
   "metadata": {},
   "source": [
    "## BERT finetune training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3858886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 1 =======\n",
      "\n",
      "Training...\n",
      "\tBatch 40 of 222, elapsed: 0:00:10\n",
      "\tBatch 80 of 222, elapsed: 0:00:18\n",
      "\tBatch 120 of 222, elapsed: 0:00:26\n",
      "\tBatch 160 of 222, elapsed: 0:00:34\n",
      "\tBatch 200 of 222, elapsed: 0:00:42\n",
      "\tBatch 222 of 222, elapsed: 0:00:47\n",
      "\n",
      "\tAverage training loss: 0.5317554961856421\n",
      "\tTraining epcoh took: 0:00:47\n",
      "\n",
      "Running validation...\n",
      "\n",
      "\tAccuracy: 0.7707894736842106\n",
      "\tValidation loss: 0.5111633718013764\n",
      "\tValidation took: 0:00:01\n",
      "\n",
      "Trainig complete!\n",
      "Total trainig took: 0:00:48\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is trained.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.train.epochs')\n",
    "conds.check(CList.BERT_SCHEDULER_CREATED)\n",
    "\n",
    "# < !!! to utils block & rename\n",
    "def flat_accuracy_bert(predictions, labels):\n",
    "    predictions_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(predictions_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# < !!! to utils block & rename\n",
    "def format_time_bert(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round(elapsed))))\n",
    "\n",
    "# < !!!\n",
    "def log_step(step: int, offset: int, length: int, t0):\n",
    "    if ((step + 1) % offset == 0 and not step == 0) or (step == length - 1):\n",
    "        elapsed = format_time_bert(time.time() - t0)\n",
    "        print(f'\\tBatch {step+1} of {len(train_dataloader)}, elapsed: {elapsed}')    \n",
    "\n",
    "# < !!!\n",
    "def extract_from_batch(batch, device) -> tuple:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    return b_input_ids, b_input_mask, b_labels\n",
    "\n",
    "bert_model.cuda()\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "train_dataloader_length = len(train_dataloader)\n",
    "val_dataloader_length = len(val_dataloader)\n",
    "\n",
    "epochs = conf('bert.train.epochs')\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'\\n======= Epoch {epoch_i + 1} / {epochs} =======\\n')\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    bert_model.train()\n",
    "        \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)\n",
    "        bert_model.zero_grad()\n",
    "        \n",
    "        res = bert_model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        loss = res['loss']\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        bert_optimizer.step()\n",
    "        bert_scheduler.step()\n",
    "        \n",
    "        log_step(step, 40, train_dataloader_length, t0)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / train_dataloader_length\n",
    "    train_time = format_time_bert(time.time() - t0)\n",
    "    print(f'\\n\\tAverage training loss: {avg_train_loss}\\n\\tTraining epcoh took: {train_time}')\n",
    "\n",
    "    print('\\nRunning validation...')\n",
    "    t0 = time.time()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    bert_model.eval()\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            res = bert_model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "        loss = res['loss']\n",
    "        logits = res['logits']\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy_bert(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / val_dataloader_length\n",
    "    avg_val_loss = total_eval_loss / val_dataloader_length\n",
    "    val_time = format_time_bert(time.time() - t0)\n",
    "    print(f'\\n\\tAccuracy: {avg_val_accuracy}')\n",
    "    print(f'\\tValidation loss: {avg_val_loss}')\n",
    "    print(f'\\tValidation took: {val_time}')\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': train_time,\n",
    "            'Validation Time': val_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('\\nTrainig complete!')\n",
    "print(f'Total trainig took: {format_time_bert(time.time() - total_t0)}')\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([i + 1 for i in range(epochs)])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "conds.set('BERT is trained.', CList.BERT_TRAINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e68d1",
   "metadata": {},
   "source": [
    "## BERT test & MCC calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530e2183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 733 of 983 (74.57%)\n",
      "Total MCC: 0.258\n",
      "BERT is tested.\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.BERT_SCHEDULER_CREATED)\n",
    "\n",
    "# < !!!\n",
    "def extract_from_batch(batch, device) -> tuple:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    return b_input_ids, b_input_mask, b_labels\n",
    "\n",
    "bert_model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('Positive samples: %d of %d (%.2f%%)' % (test_dataframe.acceptable.sum(), len(test_dataframe.acceptable), (test_dataframe.acceptable.sum() / len(test_dataframe.acceptable) * 100.0)))\n",
    "\n",
    "matthews_set = []\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "    matthews_set.append(matthews)\n",
    "\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "print('Total MCC: %.3f' % mcc)\n",
    "\n",
    "conds.set('BERT is tested.', CList.BERT_TESTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d93e8",
   "metadata": {},
   "source": [
    "## GPT preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37eb6456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT preparation is done.\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.TOKENIZERS_MODELS_CREATED)\n",
    "\n",
    "def calc_gpt_loss(text):\n",
    "    inputs = gpt_tokenizer.encode(text, return_tensors='pt').reshape(-1).to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = gpt_model(input_ids=inputs, labels=inputs).loss.item()\n",
    "    return loss\n",
    "\n",
    "def shot_gpt(begin: str, text: str, positive_statement: str, negative_statement: str):\n",
    "    positive_loss = calc_gpt_loss(' '.join([begin, text, positive_statement]))\n",
    "    negative_loss = calc_gpt_loss(' '.join([begin, text, negative_statement]))\n",
    "\n",
    "    return 1 if positive_loss > negative_loss else 0\n",
    "\n",
    "gpt_model.to(device)\n",
    "\n",
    "conds.set('GPT preparation is done.', CList.GPT_PREPARED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257b644",
   "metadata": {},
   "source": [
    "## Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a97d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [00:44<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:    ?, positive_statement:  ., negative_statement:  .\n",
      "F1-score: 0.8543123543123543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [00:44<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:    ?, positive_statement: ., negative_statement: .\n",
      "F1-score: 0.01349527665317139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [00:44<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:    ?, positive_statement: ., negative_statement: .\n",
      "F1-score: 0.13184079601990048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [00:44<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:   ., positive_statement: , negative_statement: .\n",
      "F1-score: 0.14390243902439026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [00:45<00:00, 21.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:   ., positive_statement: ., negative_statement: .\n",
      "F1-score: 0.19437939110070257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.GPT_PREPARED)\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'begin': '   ?',\n",
    "        'positive_statement': ' .',\n",
    "        'negative_statement': ' .'\n",
    "    },\n",
    "    {\n",
    "        'begin': '   ?',\n",
    "        'positive_statement': '.',\n",
    "        'negative_statement': '.'\n",
    "    },\n",
    "    {\n",
    "        'begin': '   ?',\n",
    "        'positive_statement': '.',\n",
    "        'negative_statement': '.'\n",
    "    },\n",
    "    {\n",
    "        'begin': '  .',\n",
    "        'positive_statement': '',\n",
    "        'negative_statement': '.'\n",
    "    },\n",
    "    {\n",
    "        'begin': '  .',\n",
    "        'positive_statement': '.',\n",
    "        'negative_statement': '.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# use and mcc and f1\n",
    "# < find best\n",
    "\n",
    "for task in tasks:\n",
    "    progress_function = lambda text: shot_gpt(task['begin'], text, task['positive_statement'], task['negative_statement'])\n",
    "    y_pred = test_dataframe['sentence'].progress_apply(progress_function)\n",
    "    print(f'Begin: {task[\"begin\"]}, positive_statement: {task[\"positive_statement\"]}, negative_statement: {task[\"negative_statement\"]}')\n",
    "    print(f'F1-score: {f1_score(y_pred, test_dataframe[\"acceptable\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd7db4",
   "metadata": {},
   "source": [
    "## Few shot (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d38b3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [01:10<00:00, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8543123543123543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.GPT_PREPARED)\n",
    "\n",
    "promt = \"\"\"  :\n",
    "   . => \n",
    "   ;     , , . => \n",
    "  ,    ,               . => \n",
    "\"\"\"\n",
    "\n",
    "y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => ', ' => '))\n",
    "print(f'F1-score: {f1_score(y_pred, test_dataframe[\"acceptable\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8e63b",
   "metadata": {},
   "source": [
    "## Few shot (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b5dfffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [01:11<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8543123543123543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.GPT_PREPARED)\n",
    "\n",
    "promt = \"\"\"  :\n",
    "       . => \n",
    "      ,  . => \n",
    "         . => \n",
    "       . => \n",
    "      . => \n",
    "\"\"\"\n",
    "\n",
    "y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => ', ' => '))\n",
    "print(f'F1-score: {f1_score(y_pred, test_dataframe[\"acceptable\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a5550",
   "metadata": {},
   "source": [
    "## Few shot (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c391f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 983/983 [01:47<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8543123543123543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.GPT_PREPARED)\n",
    "\n",
    "promt = \"\"\"  :\n",
    "     ,   . => \n",
    "     . => \n",
    "      . => \n",
    "     . => \n",
    "   . => \n",
    "        ,    . => \n",
    "    . => \n",
    "     ,  , ,  -   . => \n",
    "    . => \n",
    "     . => \n",
    "\"\"\"\n",
    "\n",
    "y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => ', ' => '))\n",
    "print(f'F1-score: {f1_score(y_pred, test_dataframe[\"acceptable\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd06a93",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d1cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KasymbekovPN\\AppData\\Local\\Temp\\ipykernel_11696\\1797478514.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  ACCURACY = load_metric('accuracy', keep_in_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset ./data/in_domain_train.csv is already downloaded.\n",
      " Dataset ./data/in_domain_dev.csv is already downloaded.\n",
      " Dataset ./data/out_of_domain_dev.csv is already downloaded.\n",
      " Dataset ./data/test.csv is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Python310\\lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class 'razdel.segmenters.tokenize.TokenSegmenter'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "C:\\Python310\\lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class 'razdel.segmenters.tokenize.TokenSegmenter'>: razdel.segmenters.tokenize.TokenSegmenter has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Map: 100%|| 7869/7869 [00:01<00:00, 6902.29 examples/s]\n",
      "Map: 100%|| 2787/2787 [00:00<00:00, 6304.02 examples/s]\n",
      "Map: 100%|| 2789/2789 [00:00<00:00, 6549.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 3:40:32, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-base_0.0001_0_128_0\n",
      "train {'train_runtime': 13249.8882, 'train_samples_per_second': 11.878, 'train_steps_per_second': 0.094, 'total_flos': 8984793076531200.0, 'train_loss': 0.07336962467795216, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev {'test_loss': 2.9456572292474448e-08, 'test_accuracy': 1.0, 'test_mcc': 0.0, 'test_runtime': 95.0673, 'test_samples_per_second': 29.316, 'test_steps_per_second': 0.231}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 3:53:35, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-base_0.0001_0.0001_128_0\n",
      "train {'train_runtime': 14032.2476, 'train_samples_per_second': 11.216, 'train_steps_per_second': 0.088, 'total_flos': 8984793076531200.0, 'train_loss': 0.07336962467795216, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev {'test_loss': 2.9456572292474448e-08, 'test_accuracy': 1.0, 'test_mcc': 0.0, 'test_runtime': 139.6134, 'test_samples_per_second': 19.962, 'test_steps_per_second': 0.158}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 4:25:28, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-base_0.001_0_128_0\n",
      "train {'train_runtime': 15946.0479, 'train_samples_per_second': 9.87, 'train_steps_per_second': 0.078, 'total_flos': 8984793076531200.0, 'train_loss': 0.09103632387304213, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev {'test_loss': 0.0, 'test_accuracy': 1.0, 'test_mcc': 0.0, 'test_runtime': 66.4812, 'test_samples_per_second': 41.922, 'test_steps_per_second': 0.331}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 1:27:48, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-base_0.001_0.0001_128_0\n",
      "train {'train_runtime': 5281.5862, 'train_samples_per_second': 29.798, 'train_steps_per_second': 0.235, 'total_flos': 8984793076531200.0, 'train_loss': 0.09520592424701428, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev {'test_loss': 0.0, 'test_accuracy': 1.0, 'test_mcc': 0.0, 'test_runtime': 57.6061, 'test_samples_per_second': 48.38, 'test_steps_per_second': 0.382}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "ACCURACY = load_metric('accuracy', keep_in_memory=True)\n",
    "MCC = load_metric('matthews_correlation', keep_in_memory=True)\n",
    "\n",
    "def check_or_create_directory(path: Path) -> None:\n",
    "    if not os.path.isdir(path):\n",
    "        pathlib.Path.mkdir(pathlib.Path(path))\n",
    "\n",
    "\n",
    "def load_dataset(url: str, file_path: Path) -> None:\n",
    "    file_path = str(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f' Dataset {file_path} is already downloaded.')\n",
    "    else:\n",
    "        wget.download(url, file_path)\n",
    "        print(f' Dataset {file_path} is downloaded.')\n",
    "\n",
    "\n",
    "def read_splits():    \n",
    "    train_df, in_domain_dev_df, out_of_domain_dev_df, test_df = map(\n",
    "        pd.read_csv,\n",
    "        (\n",
    "            conf('t5.dataset.path.train'),\n",
    "            conf('t5.dataset.path.in-domain-dev'),\n",
    "            conf('t5.dataset.path.out-of-domain-dev'),\n",
    "            conf('t5.dataset.path.test')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # concatenate datasets to get aggregate metrics\n",
    "    dev_df = pd.concat((in_domain_dev_df, out_of_domain_dev_df))\n",
    "\n",
    "    train, dev, test = map(Dataset.from_pandas, (train_df, dev_df, test_df))\n",
    "    return DatasetDict(train=train, dev=dev, test=test)\n",
    "\n",
    "\n",
    "def compute_metrics(p, tokenizer):\n",
    "    string_preds = tokenizer.batch_decode(p.predictions, skip_special_tokens=True)\n",
    "    int_preds = [1 if prediction == conf('t5.label.pos') else 0 for prediction in string_preds]\n",
    "\n",
    "    labels = np.where(p.label_ids != -100, p.label_ids, tokenizer.pad_token_id)\n",
    "    string_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    int_labels = []\n",
    "\n",
    "    for string_label in string_labels:\n",
    "        if string_label == conf('t5.label.pos'):\n",
    "            int_labels.append(1)\n",
    "        elif string_label == conf('t5.label.neg') or string_label == '': # second case accounts for test data\n",
    "            int_labels.append(0)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    acc_result = ACCURACY.compute(predictions=int_preds, references=int_labels)\n",
    "    mcc_result = MCC.compute(predictions=int_preds, references=int_labels)\n",
    "\n",
    "    result = {'accuracy': acc_result['accuracy'], 'mcc': mcc_result['matthews_correlation']}\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_examples(examples, tokenizer):\n",
    "    result = tokenizer(examples['sentence'], padding=True)\n",
    "\n",
    "    if 'acceptable' in examples:\n",
    "        label_sequences = []\n",
    "        for label in examples['acceptable']:\n",
    "            if label == 1:\n",
    "                target_sequence = conf('t5.label.pos')\n",
    "            elif label == 0:\n",
    "                target_sequence = conf('t5.label.pos')\n",
    "            else:\n",
    "                raise ValueError('Unknown class label')\n",
    "            label_sequences.append(target_sequence)\n",
    "    else:\n",
    "        label_sequences = ['' for _ in examples['sentence']]\n",
    "\n",
    "    result['labels'] = tokenizer(label_sequences, padding=False)['input_ids']\n",
    "    result['length'] = [len(list(tokenize(sequence))) for sequence in examples['sentence']]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_training_args(run_base_dir,\n",
    "                         batch_size,\n",
    "                         learning_rate,\n",
    "                         weight_decay,\n",
    "                         seed):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f'checkpoints/{run_base_dir}',\n",
    "        overwrite_output_dir=conf('t5.const-training-args')['overwrite_output_dir'],\n",
    "        evaluation_strategy=conf('t5.const-training-args')['evaluation_strategy'],\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=conf('t5.n-epochs'),\n",
    "        lr_scheduler_type=conf('t5.const-training-args')['lr_scheduler_type'],\n",
    "        save_strategy=conf('t5.const-training-args')['save_strategy'],\n",
    "        save_total_limit=conf('t5.const-training-args')['save_total_limit'],\n",
    "        seed=seed,\n",
    "        fp16=conf('t5.const-training-args')['fp16'],\n",
    "        dataloader_num_workers=conf('t5.const-training-args')['dataloader_num_workers'],\n",
    "        group_by_length=conf('t5.const-training-args')['group_by_length'],\n",
    "        report_to=conf('t5.const-training-args')['report_to'],\n",
    "        load_best_model_at_end=conf('t5.const-training-args')['load_best_model_at_end'],\n",
    "        metric_for_best_model=conf('t5.const-training-args')['metric_for_best_model'],\n",
    "        optim=conf('t5.const-training-args')['optim'],\n",
    "        predict_with_generate=conf('t5.const-training-args')['predict_with_generate']\n",
    "    )\n",
    "\n",
    "\n",
    "check_or_create_directory('./data')\n",
    "\n",
    "load_dataset(conf('t5.dataset.url.train'), conf('t5.dataset.path.train'))\n",
    "load_dataset(conf('t5.dataset.url.in-domain-dev'), conf('t5.dataset.path.in-domain-dev'))\n",
    "load_dataset(conf('t5.dataset.url.out-of-domain-dev'), conf('t5.dataset.path.out-of-domain-dev'))\n",
    "load_dataset(conf('t5.dataset.url.test'), conf('t5.dataset.path.test'))\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(conf('t5.pretrain.path'))\n",
    "splits = read_splits()\n",
    "\n",
    "tokenized_splits = splits.map(\n",
    "    partial(preprocess_examples, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=['sentence']\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)\n",
    "dev_metrics_per_run = np.empty((\n",
    "    conf('t5.n-seeds'),\n",
    "    len(conf('t5.lr-values')),\n",
    "    len(conf('t5.decay-vales')),\n",
    "    len(conf('t5.batch-sizes')),\n",
    "    2\n",
    "))\n",
    "\n",
    "for i, learning_rate in enumerate(conf('t5.lr-values')):\n",
    "    for j, weight_decay in enumerate(conf('t5.decay-vales')):\n",
    "        for k, batch_size in enumerate(conf('t5.batch-sizes')):\n",
    "            for seed in range(conf('t5.n-seeds')):\n",
    "                model = T5ForConditionalGeneration.from_pretrained(conf('t5.pretrain.path'))\n",
    "                model.to(device)\n",
    "                run_base_dir = f'{conf(\"t5.model-name\")}_{learning_rate}_{weight_decay}_{batch_size}'\n",
    "\n",
    "                training_args = create_training_args(run_base_dir, batch_size, learning_rate, weight_decay, seed)\n",
    "                trainer = Seq2SeqTrainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_splits['train'],\n",
    "                    eval_dataset=tokenized_splits['dev'],\n",
    "                    compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=data_collator\n",
    "                )\n",
    "\n",
    "                train_result = trainer.train()\n",
    "                print(f'{run_base_dir}_{seed}')\n",
    "                print(f'train {train_result.metrics}')\n",
    "\n",
    "                os.makedirs(f'results/{run_base_dir}_{seed}', exist_ok=True)\n",
    "\n",
    "                dev_predictions = trainer.predict(\n",
    "                    test_dataset=tokenized_splits['dev'],\n",
    "                    metric_key_prefix='test',\n",
    "                    max_length=10\n",
    "                )\n",
    "                print(f'dev {dev_predictions.metrics}')\n",
    "                dev_metrics_per_run[seed, i, j, k] = (\n",
    "                    dev_predictions.metrics['test_accuracy'],\n",
    "                    dev_predictions.metrics['test_mcc']\n",
    "                )\n",
    "\n",
    "                predictions = trainer.predict(test_dataset=tokenized_splits['test'], max_length=10)\n",
    "                string_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "                int_preds = [1 if prediction == conf('t5.label.pos') else 0 for prediction in string_preds]\n",
    "                int_preds = np.asarray(int_preds)\n",
    "\n",
    "                np.save(f'results/{run_base_dir}_{seed}/preds.npy', int_preds)\n",
    "                rmtree(f'checkpoints/{run_base_dir}')\n",
    "\n",
    "os.makedirs('result_agg', exist_ok=True)\n",
    "np.save(f'result_agg/{conf(\"t5.model-name\")}_dev.npy', dev_metrics_per_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d3480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4632547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
