{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5429398",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afc0b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d11c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a344298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration block is done.\n"
     ]
    }
   ],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'url.dataset.train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',\n",
    "            'url.dataset.test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true',\n",
    "            \n",
    "            'path.dataset.train': './train_dataset.csv',\n",
    "            'path.dataset.test': './test_dataset.csv',\n",
    "            \n",
    "            'name.train': 'TRAIN',\n",
    "            'name.test': 'TEST',\n",
    "  \n",
    "            'dataframe.train.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "#             'dataframe.train.usecols': ['sentence', 'acceptable'],\n",
    "            'dataframe.test.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "#             'dataframe.test.usecols': ['sentence', 'acceptable'],\n",
    "            \n",
    "            'bert.train-size': 0.9,\n",
    "            'bert.batch-size': 32\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args) == 0 or args[0] not in self._params:\n",
    "            return None\n",
    "        return self._params[args[0]]\n",
    "    \n",
    "    def check(self, *args):\n",
    "        result = True\n",
    "        absence_params = set()\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str) and arg not in self._params:\n",
    "                result = False\n",
    "                absence_params.add(arg)\n",
    "        message = 'Absence params: ' + ', '.join(absence_params)\n",
    "        assert result, message\n",
    "        \n",
    "        \n",
    "conf = Configurator()\n",
    "\n",
    "print('Configuration block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488cf80",
   "metadata": {},
   "source": [
    "## Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b97e7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions block is done.\n"
     ]
    }
   ],
   "source": [
    "class Conditions:\n",
    "    DEVICE_DEFINED = 'device_deviced'\n",
    "    DATASET_DOWNLOADED = 'dataset-downloaded'\n",
    "    DATASET_LOADED = 'dataset-loaded'\n",
    "    DATASET_PREPARED = 'dataset-prepared'\n",
    "    TOKENIZERS_MODELS_CREATED = 'tikenizer-models-created'\n",
    "    # collect CONDITIONS automatically !!!\n",
    "    CONDITIONS = [\n",
    "        DATASET_DOWNLOADED,\n",
    "        DATASET_LOADED,\n",
    "        DATASET_PREPARED,\n",
    "        TOKENIZERS_MODELS_CREATED,\n",
    "        DEVICE_DEFINED\n",
    "    ]\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._conditions = {}\n",
    "        \n",
    "    def set(self, *conditions):\n",
    "        for condition in conditions:\n",
    "            if condition in self.CONDITIONS:\n",
    "                self._conditions[condition] = True\n",
    "    \n",
    "    def check(self, *conditions):\n",
    "        result = True\n",
    "        absence = set()\n",
    "        for condition in conditions:\n",
    "            if condition in self.CONDITIONS and condition not in self._conditions:\n",
    "                result = False\n",
    "                absence.add(condition)\n",
    "        message = 'Absence conditions: ' + ', '.join(absence)\n",
    "        assert result, message\n",
    "    \n",
    "    \n",
    "conds = Conditions()\n",
    "\n",
    "print('Conditions block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f48c8",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "830d3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572c92a",
   "metadata": {},
   "source": [
    "## Models & tokenizers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a287bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models & tokenizers creation is done.\n"
     ]
    }
   ],
   "source": [
    "conds.check(Conditions.DEVICE_DEFINED)\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "\n",
    "conds.set(Conditions.TOKENIZERS_MODELS_CREATED)\n",
    "\n",
    "print('Models & tokenizers creation is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b4c8e",
   "metadata": {},
   "source": [
    "## Downloading datasets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88e4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"TRAIN\" is already downloaded.\n",
      "Dataset \"TEST\" is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# optimize checking  !!!\n",
    "conf.check(\n",
    "    'url.dataset.train',\n",
    "    'path.dataset.train',\n",
    "    'name.train',\n",
    "    'url.dataset.test',\n",
    "    'path.dataset.test',\n",
    "    'name.test'\n",
    ");\n",
    "conds.check(Conditions.TOKENIZERS_MODELS_CREATED)\n",
    "\n",
    "def load_dataset(url: str, path: str, name: str):\n",
    "    if os.path.exists(path):\n",
    "        print('Dataset \"' + name + '\" is already downloaded.')\n",
    "    else:\n",
    "        wget.download(url, path)\n",
    "        print(' Dataset \"' + name + '\" is downloaded.')\n",
    "        \n",
    "load_dataset(conf('url.dataset.train'), conf('path.dataset.train'), conf('name.train'))\n",
    "load_dataset(conf('url.dataset.test'), conf('path.dataset.test'), conf('name.test'))\n",
    "conds.set(Conditions.DATASET_DOWNLOADED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31748d",
   "metadata": {},
   "source": [
    "## Loading datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4e4c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "conf.check(\n",
    "    'path.dataset.train',\n",
    "    'path.dataset.test',\n",
    "    'dataframe.train.names',\n",
    "    'dataframe.train.usecols',\n",
    "    'dataframe.test.names',\n",
    "    'dataframe.test.usecols'\n",
    ")\n",
    "conds.check(Conditions.DATASET_DOWNLOADED)\n",
    "\n",
    "train_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.train'),\n",
    "    names=conf('dataframe.train.names'),\n",
    "    skip\n",
    "#     dtype= {'acceptable': np.int32},\n",
    "#     skipfooter=1,\n",
    "#     engine='python'\n",
    "#     usecols=conf('dataframe.train.usecols')\n",
    ")\n",
    "\n",
    "test_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.test'),\n",
    "    names=conf('dataframe.test.names'),\n",
    "#     usecols=conf('dataframe.test.usecols')\n",
    ")\n",
    "conds.set(Conditions.DATASET_LOADED)\n",
    "\n",
    "print('Datasets are loaded from disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d81895",
   "metadata": {},
   "source": [
    "## [bert] Datasets preparation, create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05af8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'str'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "conds.check(Conditions.DATASET_LOADED)\n",
    "\n",
    "train_sentences = train_dataframe.sentence.values[1:]\n",
    "train_acceptables = train_dataframe.acceptable.values[1:]\n",
    "test_sentences = test_dataframe.sentence.values[1:]\n",
    "test_acceptables = test_dataframe.acceptable.values[1:]\n",
    "\n",
    "\n",
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = bert_tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_sentences, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_sentences, raw_max_length)\n",
    "\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "\n",
    "max_length = define_max_length(raw_max_length, 1)\n",
    "\n",
    "def create_bert_dataset(sentences, acceptables, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = bert_tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_mask = torch.cat(attention_masks, dim=0)\n",
    "    acceptables = torch.tensor(acceptables)\n",
    "    \n",
    "    return TensorDataset(input_ids, attention_masks, acceptables)\n",
    "\n",
    "print(train_acceptables[0])\n",
    "print(type(train_acceptables[0]))\n",
    "print(type(train_acceptables))\n",
    "\n",
    "# train_val_dataset = create_bert_dataset(train_sentences, train_acceptables, max_length)\n",
    "# print(train_val_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print_header('create 90/10 train/validation split')\n",
    "#     train_size = int(0.9 * len(dataset))\n",
    "#     val_size = len(dataset) - train_size\n",
    "#     print(f'training samples: {train_size}, validation samples: {val_size}')\n",
    "#     train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "#     print_header('set batch size')\n",
    "#     batch_size = 32\n",
    "#     print(f'Batch size = {batch_size}')\n",
    "\n",
    "#     print_header('Training & validation dataloaders creation')\n",
    "#     train_dataloader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         sampler=RandomSampler(train_dataset),\n",
    "#         batch_size=batch_size\n",
    "#     )\n",
    "#     validation_dataloader = DataLoader(\n",
    "#         val_dataset,\n",
    "#         sampler=SequentialSampler(val_dataset),\n",
    "#         batch_size=batch_size\n",
    "#     )\n",
    "\n",
    "#             'bert.train-size': 0.9,\n",
    "#             'bert.batch-size': 32\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     for sentence in sentences:\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#             sentence,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=64,\n",
    "#             # pad_to_max_length=True,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "#         input_ids.append(encoded_dict['input_ids'])\n",
    "#         attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "#     print_header('Convert the lists into tensors.')\n",
    "#     input_ids = torch.cat(input_ids, dim=0)\n",
    "#     attention_masks = torch.cat(attention_masks, dim=0)\n",
    "#     labels = torch.tensor(labels)\n",
    "\n",
    "#     print_header('Set the batch size.')\n",
    "#     batch_size = 32\n",
    "#     print(f'batch size: {batch_size}')\n",
    "\n",
    "#     print_header('Create the DataLoader')\n",
    "#     prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "#     prediction_sampler = SequentialSampler(prediction_data)\n",
    "#     prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# !!! conds.check(Conditions.DATASET_PREPARED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
