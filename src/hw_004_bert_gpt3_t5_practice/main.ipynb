{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5429398",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "afc0b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import wget\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from enum import Enum\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, BertTokenizer, AutoModelForMaskedLM, BertForSequenceClassification, \\\n",
    "                         get_linear_schedule_with_warmup\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d11c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1a344298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration block is done.\n"
     ]
    }
   ],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'random.seed': 42,\n",
    "            \n",
    "            'url.dataset.train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',\n",
    "            'url.dataset.test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true',\n",
    "            \n",
    "            'path.dataset.train': './train_dataset.csv',\n",
    "            'path.dataset.test': './test_dataset.csv',\n",
    "            \n",
    "            'name.train': 'TRAIN',\n",
    "            'name.test': 'TEST',\n",
    "  \n",
    "            'dataframe.train.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "            'dataframe.test.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "            \n",
    "            'bert.train-size': 0.9,\n",
    "            'bert.batch-size': 32,\n",
    "            'bert.optimizer.learning-rate': 2e-5,\n",
    "            'bert.optimizer.eps': 1e-8,\n",
    "            'bert.train.epochs': 2\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args) == 0 or args[0] not in self._params:\n",
    "            return None\n",
    "        return self._params[args[0]]\n",
    "    \n",
    "    def check(self, *args):\n",
    "        result = True\n",
    "        absence_params = set()\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str) and arg not in self._params:\n",
    "                result = False\n",
    "                absence_params.add(arg)\n",
    "        message = 'Absence params: ' + ', '.join(absence_params)\n",
    "        assert result, message\n",
    "        \n",
    "        \n",
    "conf = Configurator()\n",
    "\n",
    "print('Configuration block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488cf80",
   "metadata": {},
   "source": [
    "## Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b97e7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions block is done.\n"
     ]
    }
   ],
   "source": [
    "class CList(Enum):\n",
    "    SET_RANDOM_SEED = 0\n",
    "    DEVICE_DEFINED = 1\n",
    "    DATASET_DOWNLOADED = 2\n",
    "    DATASET_LOADED = 3\n",
    "    DATASET_PREPARED = 4\n",
    "    TOKENIZERS_MODELS_CREATED = 5\n",
    "    BERT_SCHEDULER_CREATED = 6\n",
    "\n",
    "class Conditions:    \n",
    "    def __init__(self) -> None:\n",
    "        self._conditions = {}\n",
    "        \n",
    "    def set(self, message, *conditions):\n",
    "        for condition in conditions:\n",
    "            self._conditions[condition] = True\n",
    "        print(message)\n",
    "    \n",
    "    def check(self, *conditions):\n",
    "        result = True\n",
    "        absence = set()\n",
    "        for condition in conditions:\n",
    "            if condition not in self._conditions:\n",
    "                result = False\n",
    "                absence.add(condition.name)\n",
    "        fail_message = 'Absence conditions: ' + ', '.join(absence)\n",
    "        assert result, fail_message\n",
    "\n",
    "conds = Conditions()\n",
    "\n",
    "print('Conditions block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f9bfb",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dc0143a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "conf.check('random.seed')\n",
    "\n",
    "random.seed(conf('random.seed'))\n",
    "np.random.seed(conf('random.seed'))\n",
    "torch.manual_seed(conf('random.seed'))\n",
    "torch.cuda.manual_seed(conf('random.seed'))\n",
    "\n",
    "conds.set('Random seed is set.', CList.SET_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aeeff9",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f137c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined\n"
     ]
    }
   ],
   "source": [
    "conds.check(CList.SET_RANDOM_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "conds.set(\"Device is defined\", CList.DEVICE_DEFINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572c92a",
   "metadata": {},
   "source": [
    "## Models & tokenizers & optimizers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4a287bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models & tokenizers creation is done.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.optimizer.learning-rate', 'bert.optimizer.eps')\n",
    "conds.check(CList.DEVICE_DEFINED)\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"ai-forever/ruBert-base\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=conf('bert.optimizer.learning-rate'), eps=conf('bert.optimizer.eps'))\n",
    "\n",
    "conds.set('Models & tokenizers creation is done.', CList.TOKENIZERS_MODELS_CREATED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b4c8e",
   "metadata": {},
   "source": [
    "## Downloading datasets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e88e4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"TRAIN\" is already downloaded.\n",
      "Dataset \"TEST\" is already downloaded.\n",
      "Datasets downloading is done\n"
     ]
    }
   ],
   "source": [
    "# optimize checking  !!!\n",
    "conf.check(\n",
    "    'url.dataset.train',\n",
    "    'path.dataset.train',\n",
    "    'name.train',\n",
    "    'url.dataset.test',\n",
    "    'path.dataset.test',\n",
    "    'name.test'\n",
    ");\n",
    "conds.check(CList.TOKENIZERS_MODELS_CREATED)\n",
    "\n",
    "def load_dataset(url: str, path: str, name: str):\n",
    "    if os.path.exists(path):\n",
    "        print('Dataset \"' + name + '\" is already downloaded.')\n",
    "    else:\n",
    "        wget.download(url, path)\n",
    "        print(' Dataset \"' + name + '\" is downloaded.')\n",
    "        \n",
    "load_dataset(conf('url.dataset.train'), conf('path.dataset.train'), conf('name.train'))\n",
    "load_dataset(conf('url.dataset.test'), conf('path.dataset.test'), conf('name.test'))\n",
    "conds.set('Datasets downloading is done', CList.DATASET_DOWNLOADED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31748d",
   "metadata": {},
   "source": [
    "## Loading datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a4e4c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "conf.check(\n",
    "    'path.dataset.train',\n",
    "    'path.dataset.test',\n",
    "    'dataframe.train.names',\n",
    "    'dataframe.test.names'\n",
    ")\n",
    "conds.check(Conditions.DATASET_DOWNLOADED)\n",
    "\n",
    "train_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.train'),\n",
    "    names=conf('dataframe.train.names'),\n",
    "    skiprows=1,\n",
    "    usecols=conf('dataframe.train.usecols')\n",
    ")\n",
    "\n",
    "test_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.test'),\n",
    "    names=conf('dataframe.test.names'),\n",
    "    skiprows=1,\n",
    "    usecols=conf('dataframe.test.usecols')\n",
    ")\n",
    "conds.set(Conditions.DATASET_LOADED)\n",
    "\n",
    "print('Datasets are loaded from disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d81895",
   "metadata": {},
   "source": [
    "## BERT datasets preparation, create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "05af8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT datasets are prepared.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.batch-size', 'bert.batch-size', 'bert.batch-size')\n",
    "conds.check(Conditions.DATASET_LOADED)\n",
    "\n",
    "train_sentences = train_dataframe.sentence.values\n",
    "train_acceptables = train_dataframe.acceptable.values\n",
    "test_sentences = test_dataframe.sentence.values\n",
    "test_acceptables = test_dataframe.acceptable.values\n",
    "\n",
    "\n",
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = bert_tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_sentences, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_sentences, raw_max_length)\n",
    "\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "\n",
    "max_length = define_max_length(raw_max_length, 1)\n",
    "\n",
    "def create_bert_dataset(sentences, acceptables, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = bert_tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    acceptables = torch.tensor(acceptables)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, acceptables)\n",
    "\n",
    "train_val_dataset = create_bert_dataset(train_sentences, train_acceptables, max_length)\n",
    "test_dataset = create_bert_dataset(test_sentences, test_acceptables, max_length);\n",
    "\n",
    "train_val_dataset_size = len(train_val_dataset)\n",
    "train_size = int(conf('bert.train-size') * train_val_dataset_size)\n",
    "val_size = train_val_dataset_size - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=conf('bert.batch-size'))\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=conf('bert.batch-size'))\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=conf('bert.batch-size'))\n",
    "\n",
    "conds.set(Conditions.DATASET_PREPARED)\n",
    "print('BERT datasets are prepared.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18249531",
   "metadata": {},
   "source": [
    "## BERT scheduler creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94b02bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT scheduler is created.\n"
     ]
    }
   ],
   "source": [
    "conf.check('bert.train.epochs')\n",
    "conds.check(Conditions.DATASET_PREPARED)\n",
    "\n",
    "bert_scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * conf('bert.train.epochs')\n",
    ")\n",
    "\n",
    "conds.set(Conditions.BERT_SCHEDULER_CREATED)\n",
    "print('BERT scheduler is created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abb943",
   "metadata": {},
   "source": [
    "## BERT finetune training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8396ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conds.check(Conditions.BERT_SCHEDULER_CREATED)\n",
    "\n",
    "\n",
    "#     def flat_accuracy(predictions, labels):\n",
    "#         predictions_flat = np.argmax(predictions, axis=1).flatten()\n",
    "#         labels_flat = labels.flatten()\n",
    "#         return np.sum(predictions_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "#     def format_time(elapsed):\n",
    "#         elapsed_rounded = int(round(elapsed))\n",
    "#         return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#     print_header('fix seed')\n",
    "#     seed_val = 42\n",
    "#     random.seed(seed_val)\n",
    "#     np.random.seed(seed_val)\n",
    "#     torch.manual_seed(seed_val)\n",
    "#     torch.cuda.manual_seed(seed_val)\n",
    "\n",
    "#     print_header('train/valid')\n",
    "#     training_stats = []\n",
    "#     total_t0 = time.time()\n",
    "\n",
    "#     for epoch_i in range(0, epochs):\n",
    "#         print(\"\")\n",
    "#         print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "#         print('Training...')\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         total_train_loss = 0\n",
    "\n",
    "#         model.train()\n",
    "\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             if step % 40 == 0 and not step == 0:\n",
    "#                 elapsed = format_time(time.time() - t0)\n",
    "#                 print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "#             b_input_ids = batch[0].to(device)\n",
    "#             b_input_mask = batch[1].to(device)\n",
    "#             b_labels = batch[2].to(device)\n",
    "\n",
    "#             model.zero_grad()\n",
    "\n",
    "#             res = model(\n",
    "#                 b_input_ids,\n",
    "#                 token_type_ids=None,\n",
    "#                 attention_mask=b_input_mask,\n",
    "#                 labels=b_labels\n",
    "#             )\n",
    "#             loss = res['loss']\n",
    "#             logits = res['logits']\n",
    "\n",
    "#             total_train_loss += loss.item()\n",
    "#             loss.backward()\n",
    "\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         training_time = format_time(time.time() - t0)\n",
    "\n",
    "#         print(\"\")\n",
    "#         print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "#         print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "#         print(\"\")\n",
    "#         print(\"Running Validation...\")\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         model.eval()\n",
    "\n",
    "#         total_eval_accuracy = 0\n",
    "#         total_eval_loss = 0\n",
    "#         nb_eval_steps = 0\n",
    "\n",
    "#         for batch in validation_dataloader:\n",
    "#             b_input_ids = batch[0].to(device)\n",
    "#             b_input_mask = batch[1].to(device)\n",
    "#             b_labels = batch[2].to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 res = model(\n",
    "#                     b_input_ids,\n",
    "#                     token_type_ids=None,\n",
    "#                     attention_mask=b_input_mask,\n",
    "#                     labels=b_labels\n",
    "#                 )\n",
    "#             loss = res['loss']\n",
    "#             logits = res['logits']\n",
    "#             total_eval_loss += loss.item()\n",
    "\n",
    "#             logits = logits.detach().cpu().numpy()\n",
    "#             label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "#             total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "#         avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "#         print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "#         avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "#         validation_time = format_time(time.time() - t0)\n",
    "\n",
    "#         print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "#         print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "#         training_stats.append(\n",
    "#             {\n",
    "#                 'epoch': epoch_i + 1,\n",
    "#                 'Training Loss': avg_train_loss,\n",
    "#                 'Valid. Loss': avg_val_loss,\n",
    "#                 'Valid. Accur.': avg_val_accuracy,\n",
    "#                 'Training Time': training_time,\n",
    "#                 'Validation Time': validation_time\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"Training complete!\")\n",
    "#     print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "#     print_header('display stats')\n",
    "#     df_stats = pd.DataFrame(data=training_stats)\n",
    "#     df_stats = df_stats.set_index('epoch')\n",
    "#     print(df_stats)\n",
    "\n",
    "#     print('Display in plot')\n",
    "#     sns.set(style='darkgrid')\n",
    "#     sns.set(font_scale=1.5)\n",
    "#     plt.rcParams['figure.figsize'] = (12, 6)\n",
    "#     plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "#     plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "#     plt.title(\"Training & Validation Loss\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.legend()\n",
    "#     plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
