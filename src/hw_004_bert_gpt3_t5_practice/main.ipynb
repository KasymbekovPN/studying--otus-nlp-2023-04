{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5429398",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc0b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import random\n",
    "import wget\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from shutil import rmtree\n",
    "from razdel import tokenize\n",
    "from datasets import (\n",
    "    load_metric,\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import (\n",
    "    random_split,\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    T5Tokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d11c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a344298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration block is done.\n"
     ]
    }
   ],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'random': {'seed': 42},\n",
    "            \n",
    "            'pretrained': {\n",
    "                'path': {\n",
    "                    'bert': 'ai-forever/ruBert-base',\n",
    "                    'gpt': 'ai-forever/rugpt3large_based_on_gpt2',\n",
    "                    't5': 'ai-forever/ruT5-base',\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'optimizer': {\n",
    "                'lr': {\n",
    "                    'bert': 2e-5,\n",
    "                    't5': 2e-5\n",
    "                },\n",
    "                'eps': {\n",
    "                    'bert': 1e-8,\n",
    "                    't5': 1e-8\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'dataset': {\n",
    "                'url': {\n",
    "                    'train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',\n",
    "                    'test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true'\n",
    "                },\n",
    "                'path': {\n",
    "                    'train': './train_dataset.csv',\n",
    "                    'test': './test_dataset.csv'\n",
    "                },\n",
    "                'name': {\n",
    "                    'train': 'TRAIN',\n",
    "                    'test': 'TEST'\n",
    "                },\n",
    "                'usecols': [1, 2]\n",
    "            },\n",
    "            \n",
    "            'train': {\n",
    "                'size': 0.9,\n",
    "                'epochs': {\n",
    "                    'bert': 1,\n",
    "                    't5': 2\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'batch-size': 32\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        param_name = args[0]\n",
    "        path = param_name.split('.')\n",
    "        return self._get_next(path, self._params, param_name)\n",
    "        \n",
    "    def _get_next(self, path, params, param_name):\n",
    "        result = None\n",
    "        if isinstance(params, dict) and len(path) > 0:\n",
    "            key = path.pop(0)\n",
    "            if len(path) == 0:\n",
    "                if key in params:\n",
    "                    result = params[key]\n",
    "            else:\n",
    "                return self._get_next(path, params[key], param_name)\n",
    "        if result is None:\n",
    "            print(f'Bad param name: {param_name}')\n",
    "        return result\n",
    "        \n",
    "conf = Configurator()\n",
    "\n",
    "print('Configuration block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b1e38",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6121820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set.\n"
     ]
    }
   ],
   "source": [
    "random.seed(conf('random.seed'))\n",
    "np.random.seed(conf('random.seed'))\n",
    "torch.manual_seed(conf('random.seed'))\n",
    "torch.cuda.manual_seed(conf('random.seed'))\n",
    "\n",
    "print('Random seed is set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41acee44",
   "metadata": {},
   "source": [
    "## Initialize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534bb5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total result is inizialized.\n"
     ]
    }
   ],
   "source": [
    "total_result = {\n",
    "    'bert': None,\n",
    "    'gpt-zero-shot': None,\n",
    "    'gpt-few-shot-3': None,\n",
    "    'gpt-few-shot-5': None,\n",
    "    'gpt-few-shot-10': None,\n",
    "    't5': None\n",
    "}\n",
    "\n",
    "print('Total result is inizialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aeeff9",
   "metadata": {},
   "source": [
    "## Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f137c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Device is defined.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU available, using the GPU instead.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Device is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572c92a",
   "metadata": {},
   "source": [
    "## Models & tokenizers & optimizers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a287bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models, tokenizers, optimizers are created.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(conf('pretrained.path.bert'))\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    conf('pretrained.path.bert'),\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=conf('optimizer.lr.bert'), eps=conf('optimizer.eps.bert'))\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(conf('pretrained.path.gpt'))\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(conf('pretrained.path.gpt'))\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(conf('pretrained.path.t5'), use_fast=False)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(conf('pretrained.path.t5'))\n",
    "t5_optimizer = AdamW(t5_model.parameters(), lr=conf('optimizer.lr.t5'), eps=conf('optimizer.eps.t5'))\n",
    "\n",
    "print('Models, tokenizers, optimizers are created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b4c8e",
   "metadata": {},
   "source": [
    "## Downloading datasets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88e4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"TRAIN\" is already downloaded.\n",
      "Dataset \"TEST\" is already downloaded.\n",
      "Datasets are downloaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_dataset(url: str, path: str, name: str):\n",
    "    if os.path.exists(path):\n",
    "        print('Dataset \"' + name + '\" is already downloaded.')\n",
    "    else:\n",
    "        wget.download(url, path)\n",
    "        print(' Dataset \"' + name + '\" is downloaded.')\n",
    "        \n",
    "load_dataset(conf('dataset.url.train'), conf('dataset.path.train'), conf('dataset.name.train'))\n",
    "load_dataset(conf('dataset.url.test'), conf('dataset.path.test'), conf('dataset.name.test'))\n",
    "\n",
    "print('Datasets are downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31748d",
   "metadata": {},
   "source": [
    "## Loading dataframes from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e4c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes are loaded.\n",
      "Train dataframe size: 7082\n",
      "Evaluate dataframe size: 787\n",
      "Test dataframe size: 983\n"
     ]
    }
   ],
   "source": [
    "train_eval_dataframe = pd.read_csv(conf('dataset.path.train'), usecols=conf('dataset.usecols'))\n",
    "test_dataframe = pd.read_csv(conf('dataset.path.test'), usecols=conf('dataset.usecols'))\n",
    "\n",
    "random_index = train_eval_dataframe.sample(frac=conf('train.size'), random_state=conf('random.seed')).index\n",
    "eval_dataframe = train_eval_dataframe[~train_eval_dataframe.index.isin(random_index)]\n",
    "train_dataframe = train_eval_dataframe[train_eval_dataframe.index.isin(random_index)]\n",
    "\n",
    "print('Dataframes are loaded.')\n",
    "print(f'Train dataframe size: {len(train_dataframe)}')\n",
    "print(f'Evaluate dataframe size: {len(eval_dataframe)}')\n",
    "print(f'Test dataframe size: {len(test_dataframe)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de202ad",
   "metadata": {},
   "source": [
    "## Define max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8853c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sentences is defined.\n",
      "Raw max_length is 45\n",
      "Binary-based max_length is 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def define_raw_max_length_by_bert(sentences, raw_max_length):\n",
    "    for sentence in sentences:\n",
    "        input_ids = bert_tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        raw_max_length = max(raw_max_length, len(input_ids))    \n",
    "    return raw_max_length\n",
    "\n",
    "def define_max_length(raw_max_length, threshold):\n",
    "    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n",
    "\n",
    "raw_max_length = define_raw_max_length_by_bert(train_dataframe.sentence.values, 0)\n",
    "raw_max_length = define_raw_max_length_by_bert(eval_dataframe.sentence.values, raw_max_length)\n",
    "raw_max_length = define_raw_max_length_by_bert(test_dataframe.sentence.values, raw_max_length)\n",
    "\n",
    "max_length = define_max_length(raw_max_length, 1)\n",
    "\n",
    "print('Maximum length of sentences is defined.')\n",
    "print(f'Raw max_length is {raw_max_length}')\n",
    "print(f'Binary-based max_length is {max_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d81895",
   "metadata": {},
   "source": [
    "## Datasets & dataloaders creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05af8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 733 of 983 (74.57%)\n",
      "BERT & T5 dataloaders are created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class T5TestDataset(TorchDataset):\n",
    "    def __init__(self, text, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._text.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item])\n",
    "        return {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=self._length)\n",
    "\n",
    "\n",
    "class T5TrainDataset(TorchDataset):\n",
    "    POS_LABEL = 'верно'\n",
    "    NEG_LABEL = 'неверно'\n",
    "\n",
    "    def __init__(self, text, label, tokenizer, length, device):\n",
    "        self._text = text.reset_index(drop=True)\n",
    "        self._label = label.reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._length = length\n",
    "        self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._label.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = self._tokenize(self._text[item], self._length)\n",
    "        output = {k: v.reshape(-1).to(self._device) for k, v in output.items()}\n",
    "\n",
    "        label = self.POS_LABEL if self._label[item] == 1 else self.NEG_LABEL\n",
    "        label = self._tokenize(label, length=2).input_ids.reshape(-1).to(self._device)\n",
    "\n",
    "        output.update({'labels': label})\n",
    "        return output\n",
    "\n",
    "    def _tokenize(self, text, length):\n",
    "        return self._tokenizer(text,\n",
    "                               return_tensors='pt',\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=length)\n",
    "\n",
    "    \n",
    "def create_bert_dataset(sentences, acceptables, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = bert_tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    acceptables = torch.tensor(acceptables)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, acceptables)\n",
    "\n",
    "test_acceptable = test_dataframe.acceptable\n",
    "print(f'Positive samples: {test_acceptable.sum()} of {len(test_acceptable)} ({100.0*test_acceptable.sum()/len(test_acceptable):.2f}%)')\n",
    "\n",
    "train_sentences = train_dataframe.sentence.values\n",
    "train_acceptables = train_dataframe.acceptable.values\n",
    "eval_sentences = eval_dataframe.sentence.values\n",
    "eval_acceptables = eval_dataframe.acceptable.values\n",
    "test_sentences = test_dataframe.sentence.values\n",
    "test_acceptables = test_dataframe.acceptable.values\n",
    "\n",
    "bert_train_dataset = create_bert_dataset(train_sentences, train_acceptables, max_length)\n",
    "bert_eval_dataset = create_bert_dataset(eval_sentences, eval_acceptables, max_length)\n",
    "bert_test_dataset = create_bert_dataset(test_sentences, test_acceptables, max_length)\n",
    "\n",
    "t5_train_dataset = T5TrainDataset(train_dataframe['sentence'], train_dataframe['acceptable'], t5_tokenizer, max_length, device)\n",
    "t5_eval_dataset = T5TrainDataset(eval_dataframe['sentence'], eval_dataframe['acceptable'], t5_tokenizer, max_length, device)\n",
    "t5_test_dataset = T5TestDataset(test_dataframe['sentence'], t5_tokenizer, max_length, device)\n",
    "\n",
    "bert_train_dataloader = DataLoader(\n",
    "    bert_train_dataset,\n",
    "    sampler=RandomSampler(bert_train_dataset),\n",
    "    batch_size=conf('batch-size')\n",
    ")\n",
    "bert_eval_dataloader = DataLoader(\n",
    "    bert_eval_dataset,\n",
    "    sampler=SequentialSampler(bert_eval_dataset),\n",
    "    batch_size=conf('batch-size')\n",
    ")\n",
    "bert_test_dataloader = DataLoader(\n",
    "    bert_test_dataset,\n",
    "    sampler=SequentialSampler(bert_test_dataset),\n",
    "    batch_size=conf('batch-size')\n",
    ")\n",
    "\n",
    "t5_train_dataloader = DataLoader(t5_train_dataset, batch_size=conf('batch-size'), shuffle=True)\n",
    "t5_eval_dataloader = DataLoader(t5_eval_dataset, batch_size=conf('batch-size'))\n",
    "t5_test_dataloader = DataLoader(t5_test_dataset, batch_size=conf('batch-size'))\n",
    "\n",
    "print('BERT & T5 dataloaders are created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ae05e",
   "metadata": {},
   "source": [
    "## Schedulers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c4a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT & T5 schedulers are created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(bert_train_dataloader) * conf('train.epochs.bert')\n",
    ")\n",
    "\n",
    "t5_scheduler = get_linear_schedule_with_warmup(\n",
    "    t5_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(t5_train_dataloader) * conf('train.epochs.t5')\n",
    ")\n",
    "\n",
    "print('BERT & T5 schedulers are created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe30d1",
   "metadata": {},
   "source": [
    "## BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3858886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 1 =======\n",
      "\n",
      "Training...\n",
      "\tBatch 40 of 222, elapsed: 0:00:10\n",
      "\tBatch 80 of 222, elapsed: 0:00:18\n",
      "\tBatch 120 of 222, elapsed: 0:00:26\n",
      "\tBatch 160 of 222, elapsed: 0:00:34\n",
      "\tBatch 200 of 222, elapsed: 0:00:43\n",
      "\tBatch 222 of 222, elapsed: 0:00:47\n",
      "\n",
      "\tAverage training loss: 0.5359252442916235\n",
      "\tTraining epcoh took: 0:00:47\n",
      "\n",
      "Running validation...\n",
      "\n",
      "\tAccuracy: 0.76875\n",
      "\tValidation loss: 0.5093031179904938\n",
      "\tValidation took: 0:00:01\n",
      "\n",
      "Trainig complete!\n",
      "Total trainig took: 0:00:49\n"
     ]
    }
   ],
   "source": [
    "def flat_accuracy_bert(predictions, labels):\n",
    "    predictions_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(predictions_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time_bert(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round(elapsed))))\n",
    "\n",
    "def log_step(step: int, offset: int, length: int, t0):\n",
    "    if ((step + 1) % offset == 0 and not step == 0) or (step == length - 1):\n",
    "        elapsed = format_time_bert(time.time() - t0)\n",
    "        print(f'\\tBatch {step+1} of {len(bert_train_dataloader)}, elapsed: {elapsed}')    \n",
    "\n",
    "def extract_from_batch(batch, device) -> tuple:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    return b_input_ids, b_input_mask, b_labels\n",
    "\n",
    "bert_model.cuda()\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "train_dataloader_length = len(bert_train_dataloader)\n",
    "val_dataloader_length = len(bert_eval_dataloader)\n",
    "\n",
    "epochs = conf('train.epochs.bert')\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'\\n======= Epoch {epoch_i + 1} / {epochs} =======\\n')\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    bert_model.train()\n",
    "        \n",
    "    for step, batch in enumerate(bert_train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)\n",
    "        bert_model.zero_grad()\n",
    "        \n",
    "        res = bert_model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        loss = res['loss']\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        bert_optimizer.step()\n",
    "        bert_scheduler.step()\n",
    "        \n",
    "        log_step(step, 40, train_dataloader_length, t0)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / train_dataloader_length\n",
    "    train_time = format_time_bert(time.time() - t0)\n",
    "    print(f'\\n\\tAverage training loss: {avg_train_loss}\\n\\tTraining epcoh took: {train_time}')\n",
    "\n",
    "    print('\\nRunning validation...')\n",
    "    t0 = time.time()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    bert_model.eval()\n",
    "    \n",
    "    for batch in bert_eval_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            res = bert_model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "        loss = res['loss']\n",
    "        logits = res['logits']\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy_bert(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / val_dataloader_length\n",
    "    avg_val_loss = total_eval_loss / val_dataloader_length\n",
    "    val_time = format_time_bert(time.time() - t0)\n",
    "    print(f'\\n\\tAccuracy: {avg_val_accuracy}')\n",
    "    print(f'\\tValidation loss: {avg_val_loss}')\n",
    "    print(f'\\tValidation took: {val_time}')\n",
    "\n",
    "print('\\nTrainig complete!')\n",
    "print(f'Total trainig took: {format_time_bert(time.time() - total_t0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e68d1",
   "metadata": {},
   "source": [
    "## BERT  testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "530e2183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is tested, F1-score: 0.867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in bert_test_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "f1 = f1_score(flat_true_labels, flat_predictions)\n",
    "total_result['bert'] = f1\n",
    "print(f'BERT is tested, F1-score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d93e8",
   "metadata": {},
   "source": [
    "## GPT preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37eb6456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT preparation is done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_gpt_loss(text):\n",
    "    inputs = gpt_tokenizer.encode(text, return_tensors='pt').reshape(-1).to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = gpt_model(input_ids=inputs, labels=inputs).loss.item()\n",
    "    return loss\n",
    "\n",
    "def shot_gpt(begin: str, text: str, positive_statement: str, negative_statement: str):\n",
    "    positive_loss = calc_gpt_loss(' '.join([begin, text, positive_statement]))\n",
    "    negative_loss = calc_gpt_loss(' '.join([begin, text, negative_statement]))\n",
    "\n",
    "    return 1 if positive_loss > negative_loss else 0\n",
    "\n",
    "gpt_model.to(device)\n",
    "\n",
    "print('GPT preparation is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257b644",
   "metadata": {},
   "source": [
    "## GPT zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a97d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin: Если ли здесь ошибка?, positive_statement: Нет., negative_statement: Есть.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.01349527665317139\n",
      "\n",
      "Begin: Если ли здесь ошибка?, positive_statement: Отсутствует., negative_statement: Присутствует.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.13184079601990048\n",
      "\n",
      "Begin: Если ли здесь ошибка?, positive_statement: Предложение правильное., negative_statement: Допущена ошибка.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8543123543123543\n",
      "\n",
      "Begin: Проверь на ошибки., positive_statement: Нет, negative_statement: Есть.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.14390243902439026\n",
      "\n",
      "Begin: Проверь на ошибки., positive_statement: Отсутствуют., negative_statement: Присутствуют.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.19437939110070257\n",
      "\n",
      "\n",
      "GPT zero-shot is done, best F1-score: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'begin': 'Если ли здесь ошибка?',\n",
    "        'positive_statement': 'Нет.',\n",
    "        'negative_statement': 'Есть.'\n",
    "    },\n",
    "    {\n",
    "        'begin': 'Если ли здесь ошибка?',\n",
    "        'positive_statement': 'Отсутствует.',\n",
    "        'negative_statement': 'Присутствует.'\n",
    "    },\n",
    "    {\n",
    "        'begin': 'Если ли здесь ошибка?',\n",
    "        'positive_statement': 'Предложение правильное.',\n",
    "        'negative_statement': 'Допущена ошибка.'\n",
    "    },\n",
    "    {\n",
    "        'begin': 'Проверь на ошибки.',\n",
    "        'positive_statement': 'Нет',\n",
    "        'negative_statement': 'Есть.'\n",
    "    },\n",
    "    {\n",
    "        'begin': 'Проверь на ошибки.',\n",
    "        'positive_statement': 'Отсутствуют.',\n",
    "        'negative_statement': 'Присутствуют.'\n",
    "    }\n",
    "]\n",
    "\n",
    "best_f1 = 0.0\n",
    "for task in tasks:\n",
    "    print(f'Begin: {task[\"begin\"]}, positive_statement: {task[\"positive_statement\"]}, negative_statement: {task[\"negative_statement\"]}')\n",
    "    progress_function = lambda text: shot_gpt(task['begin'], text, task['positive_statement'], task['negative_statement'])\n",
    "    y_pred = test_dataframe['sentence'].progress_apply(progress_function)\n",
    "    f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n",
    "    best_f1 = max(best_f1, f1)\n",
    "    print(f'F1-score: {f1}\\n')\n",
    "    \n",
    "total_result['gpt-zero-shot'] = best_f1\n",
    "print(f'\\nGPT zero-shot is done, best F1-score: {best_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd7db4",
   "metadata": {},
   "source": [
    "## GPT few shot (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d38b3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [01:11<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT few-shot(3) is done, F1-score: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "promt = \"\"\"Проверить корректность предложения:\n",
    "Иван вчера не позвонил. => Верно\n",
    "Лесные запахи набегали волнами; в них смешалось дыхание можжевельника, вереска, брусники. => Верно\n",
    "У многих туристов, кто посещают Кемер весной, есть шанс застать снег на вершине горы Тахталы и даже сочетать пляжный отдых с горнолыжным. => Неверно\n",
    "\"\"\"\n",
    "\n",
    "y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => Верно', ' => Неверно'))\n",
    "f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n",
    "total_result['gpt-few-shot-3'] = f1\n",
    "print(f'GPT few-shot(3) is done, F1-score: {f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8e63b",
   "metadata": {},
   "source": [
    "## GPT few shot (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5dfffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [01:11<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT few-shot(5) is done, F1-score: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "promt = \"\"\"Проверить корректность предложения:\n",
    "Вчера президент имел неофициальную беседу с английским послом. => Верно\n",
    "А ты ехай прямо к директору театров, князю Гагарину. => Неверно\n",
    "Коллега так и не признал вину за катастрофу перед коллективом. => Верно\n",
    "Малыш уже уверенно читает слова через мягкий знак. => Неверно\n",
    "Я говорил с ним только ради Вас. => Верно\n",
    "\"\"\"\n",
    "\n",
    "y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => Верно', ' => Неверно'))\n",
    "f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n",
    "total_result['gpt-few-shot-5'] = f1\n",
    "print(f'GPT few-shot(5) is done, F1-score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a5550",
   "metadata": {},
   "source": [
    "## GPT few shot (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c391f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [01:48<00:00,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT few-shot(10) is done, F1-score: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "promt = \"\"\"Проверить корректность предложения:\n",
    "Только бы он громко не закричал, когда найдет решение. => Верно\n",
    "Но Коле не помог его иноверец. => Неверно\n",
    "Гармоничные пропорции здания основаны на классических образцах. => Верно\n",
    "Вчера мне нужно ехать на завод. => Неверно\n",
    "Дело приняло дурной оборот. => Верно\n",
    "Я стою поблизости от Центризбиркома и наблюдаю за тем, что происходит у входа. => Неверно\n",
    "Вани не было в школе. => Верно\n",
    "В качестве примеров были приведены случаи, о которых, кажется, что где-то я уже читал. => Неверно\n",
    "Кожа у виска была желтой. => Верно\n",
    "В эту минуту роман был прочитан. => Неверно\n",
    "\"\"\"\n",
    "\n",
    "y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => Верно', ' => Неверно'))\n",
    "f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n",
    "total_result['gpt-few-shot-10'] = f1\n",
    "print(f'GPT few-shot(10) is done, F1-score: {f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd06a93",
   "metadata": {},
   "source": [
    "## T5 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60d1cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 2 =======\n",
      "\n",
      "Training...\n",
      "\n",
      "\t\tBatch 10 of 222, loss : 5.728\n",
      "\t\tBatch 20 of 222, loss : 2.136\n",
      "\t\tBatch 30 of 222, loss : 1.363\n",
      "\t\tBatch 40 of 222, loss : 0.833\n",
      "\t\tBatch 50 of 222, loss : 0.582\n",
      "\t\tBatch 60 of 222, loss : 0.552\n",
      "\t\tBatch 70 of 222, loss : 0.529\n",
      "\t\tBatch 80 of 222, loss : 0.374\n",
      "\t\tBatch 90 of 222, loss : 0.644\n",
      "\t\tBatch 100 of 222, loss : 0.402\n",
      "\t\tBatch 110 of 222, loss : 0.395\n",
      "\t\tBatch 120 of 222, loss : 0.370\n",
      "\t\tBatch 130 of 222, loss : 0.294\n",
      "\t\tBatch 140 of 222, loss : 0.499\n",
      "\t\tBatch 150 of 222, loss : 0.300\n",
      "\t\tBatch 160 of 222, loss : 0.430\n",
      "\t\tBatch 170 of 222, loss : 0.437\n",
      "\t\tBatch 180 of 222, loss : 0.233\n",
      "\t\tBatch 190 of 222, loss : 0.542\n",
      "\t\tBatch 200 of 222, loss : 0.252\n",
      "\t\tBatch 210 of 222, loss : 0.401\n",
      "\t\tBatch 220 of 222, loss : 0.194\n",
      "\t\tBatch 222 of 222, loss : 0.191\n",
      "Validation...\n",
      "\tValidation loss: 0.2917381364107132\n",
      "\n",
      "======= Epoch 2 / 2 =======\n",
      "\n",
      "Training...\n",
      "\n",
      "\t\tBatch 10 of 222, loss : 0.354\n",
      "\t\tBatch 20 of 222, loss : 0.500\n",
      "\t\tBatch 30 of 222, loss : 0.333\n",
      "\t\tBatch 40 of 222, loss : 0.327\n",
      "\t\tBatch 50 of 222, loss : 0.284\n",
      "\t\tBatch 60 of 222, loss : 0.403\n",
      "\t\tBatch 70 of 222, loss : 0.352\n",
      "\t\tBatch 80 of 222, loss : 0.358\n",
      "\t\tBatch 90 of 222, loss : 0.466\n",
      "\t\tBatch 100 of 222, loss : 0.378\n",
      "\t\tBatch 110 of 222, loss : 0.410\n",
      "\t\tBatch 120 of 222, loss : 0.325\n",
      "\t\tBatch 130 of 222, loss : 0.271\n",
      "\t\tBatch 140 of 222, loss : 0.211\n",
      "\t\tBatch 150 of 222, loss : 0.287\n",
      "\t\tBatch 160 of 222, loss : 0.307\n",
      "\t\tBatch 170 of 222, loss : 0.427\n",
      "\t\tBatch 180 of 222, loss : 0.253\n",
      "\t\tBatch 190 of 222, loss : 0.376\n",
      "\t\tBatch 200 of 222, loss : 0.354\n",
      "\t\tBatch 210 of 222, loss : 0.290\n",
      "\t\tBatch 220 of 222, loss : 0.393\n",
      "\t\tBatch 222 of 222, loss : 0.230\n",
      "Validation...\n",
      "\tValidation loss: 0.290314502120018\n",
      "Trainig complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t5_model.to(device)\n",
    "n_epochs = conf('train.epochs.t5')\n",
    "\n",
    "dl_length = len(t5_train_dataloader)\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'\\n======= Epoch {epoch + 1} / {n_epochs} =======\\n')\n",
    "\n",
    "    print('Training...\\n')\n",
    "    t5_model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(t5_train_dataloader):\n",
    "        outputs = t5_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        t5_optimizer.step()\n",
    "        t5_scheduler.step()\n",
    "        t5_optimizer.zero_grad()\n",
    "        if (((batch_id + 1) % 10 == 0) and not batch_id == 0) or (batch_id == dl_length - 1):\n",
    "            print(f'\\t\\tBatch {batch_id+1} of {dl_length}, loss : {loss.item():.3f}')\n",
    "        \n",
    "    print('Validation...')\n",
    "    t5_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loss = [t5_model(**batch).loss.item() for batch in t5_eval_dataloader]\n",
    "        \n",
    "    print(f'\\tValidation loss: {np.sum(eval_loss)/len(eval_loss)}')\n",
    "    \n",
    "print('Trainig complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17a694",
   "metadata": {},
   "source": [
    "## T5 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0ff86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 testing is done, F1-score: 0.854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_label = t5_tokenizer(T5TrainDataset.POS_LABEL,\n",
    "                         return_tensors='pt',\n",
    "                         padding='max_length',\n",
    "                         truncation=True,\n",
    "                         max_length=2)['input_ids'][0][0].item()\n",
    "\n",
    "t5_model.eval()\n",
    "\n",
    "result = np.array([])\n",
    "for batch in t5_test_dataloader:\n",
    "    tokens = t5_model.generate(**batch)\n",
    "    tokens = [1 if pos_label in token else 0 for token in tokens]\n",
    "    result = np.hstack([result, tokens])\n",
    "\n",
    "f1 = f1_score(result, test_dataframe[\"acceptable\"])\n",
    "total_result['t5'] = f1\n",
    "\n",
    "print(f'T5 testing is done, F1-score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2291480",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4632547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>bert</td>\n",
       "      <td>0.867310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-zero-shot</td>\n",
       "      <td>0.854312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-few-shot-3</td>\n",
       "      <td>0.854312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-few-shot-5</td>\n",
       "      <td>0.854312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gpt-few-shot-10</td>\n",
       "      <td>0.854312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t5</td>\n",
       "      <td>0.854142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[k, v] for k, v in total_result.items()]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Type', 'F1'])\n",
    "HTML(df.to_html(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
