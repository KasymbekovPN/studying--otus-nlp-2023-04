{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5429398",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afc0b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import is done.\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "print('Import is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089c11e",
   "metadata": {},
   "source": [
    "## Models & tokenizers creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████| 590/590 [00:00<?, ?B/s]\n",
      "C:\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KasymbekovPN\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████████████████████| 1.78M/1.78M [00:02<00:00, 609kB/s]\n",
      "Downloading pytorch_model.bin:   4%|██                                              | 31.5M/716M [00:44<15:43, 726kB/s]"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(\"ai-forever/ruBert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d11c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a344298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration block is done.\n"
     ]
    }
   ],
   "source": [
    "class Configurator:\n",
    "    def __init__(self) -> None:\n",
    "        self._params = {\n",
    "            'url.dataset.train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',\n",
    "            'url.dataset.test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true',\n",
    "            \n",
    "            'path.dataset.train': './train_dataset.csv',\n",
    "            'path.dataset.test': './test_dataset.csv',\n",
    "            \n",
    "            'name.train': 'TRAIN',\n",
    "            'name.test': 'TEST',\n",
    "  \n",
    "            'dataframe.train.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "            'dataframe.train.usecols': ['sentence', 'acceptable'],\n",
    "            'dataframe.test.names': ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source'],\n",
    "            'dataframe.test.usecols': ['sentence', 'acceptable'],\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args) == 0 or args[0] not in self._params:\n",
    "            return None\n",
    "        return self._params[args[0]]\n",
    "    \n",
    "    def check(self, *args):\n",
    "        result = True\n",
    "        absence_params = set()\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str) and arg not in self._params:\n",
    "                result = False\n",
    "                absence_params.add(arg)\n",
    "        message = 'Absence params: ' + ', '.join(absence_params)\n",
    "        assert result, message\n",
    "        \n",
    "        \n",
    "conf = Configurator()\n",
    "\n",
    "print('Configuration block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488cf80",
   "metadata": {},
   "source": [
    "## Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b97e7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions block is done.\n"
     ]
    }
   ],
   "source": [
    "class Conditions:\n",
    "    DATASET_DOWNLOADED = 'dataset-downloaded'\n",
    "    DATASET_LOADED = 'dataset-loaded'\n",
    "    DATASET_PREPARED = 'dataset-prepared'\n",
    "    # collect CONDITIONS automatically !!!\n",
    "    CONDITIONS = [\n",
    "        DATASET_DOWNLOADED,\n",
    "        DATASET_LOADED,\n",
    "        DATASET_PREPARED\n",
    "    ]\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._conditions = {}\n",
    "        \n",
    "    def set(self, *conditions):\n",
    "        for condition in conditions:\n",
    "            if condition in self.CONDITIONS:\n",
    "                self._conditions[condition] = True\n",
    "    \n",
    "    def check(self, *conditions):\n",
    "        result = True\n",
    "        absence = set()\n",
    "        for condition in conditions:\n",
    "            if condition in self.CONDITIONS and condition not in self._conditions:\n",
    "                result = False\n",
    "                absence.add(condition)\n",
    "        message = 'Absence conditions: ' + ', '.join(absence)\n",
    "        assert result, message\n",
    "    \n",
    "    \n",
    "conds = Conditions()\n",
    "\n",
    "print('Conditions block is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b4c8e",
   "metadata": {},
   "source": [
    "## Downloading datasets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e88e4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"TRAIN\" is already downloaded.\n",
      "Dataset \"TEST\" is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "conf.check(\n",
    "    'url.dataset.train',\n",
    "    'path.dataset.train',\n",
    "    'name.train',\n",
    "    'url.dataset.test',\n",
    "    'path.dataset.test',\n",
    "    'name.test'\n",
    ");\n",
    "\n",
    "def load_dataset(url: str, path: str, name: str):\n",
    "    if os.path.exists(path):\n",
    "        print('Dataset \"' + name + '\" is already downloaded.')\n",
    "    else:\n",
    "        wget.download(url, path)\n",
    "        print(' Dataset \"' + name + '\" is downloaded.')\n",
    "        \n",
    "load_dataset(conf('url.dataset.train'), conf('path.dataset.train'), conf('name.train'))\n",
    "load_dataset(conf('url.dataset.test'), conf('path.dataset.test'), conf('name.test'))\n",
    "conds.set(Conditions.DATASET_DOWNLOADED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4daf5b3",
   "metadata": {},
   "source": [
    "## Loading datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4e4c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "conf.check(\n",
    "    'path.dataset.train',\n",
    "    'path.dataset.test',\n",
    "    'dataframe.train.names',\n",
    "    'dataframe.train.usecols',\n",
    "    'dataframe.test.names',\n",
    "    'dataframe.test.usecols'\n",
    ")\n",
    "conds.check(Conditions.DATASET_DOWNLOADED)\n",
    "\n",
    "train_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.train'),\n",
    "    names=conf('dataframe.train.names'),\n",
    "    usecols=conf('dataframe.train.usecols')\n",
    ")\n",
    "\n",
    "test_dataframe = pd.read_csv(\n",
    "    conf('path.dataset.test'),\n",
    "    names=conf('dataframe.test.names'),\n",
    "    usecols=conf('dataframe.test.usecols')\n",
    ")\n",
    "conds.set(Conditions.DATASET_LOADED)\n",
    "\n",
    "print('Datasets are loaded from disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b267d",
   "metadata": {},
   "source": [
    "## Datasets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f08a3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conds.check(Conditions.DATASET_LOADED)\n",
    "\n",
    "# !!! conds.check(Conditions.DATASET_PREPARED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
